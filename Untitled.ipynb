{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e58301e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - d_loss: 83.048, d_acc: 0.047, g_loss: 0.248\n",
      "Epoch 2/5 - d_loss: 4.758, d_acc: 0.500, g_loss: 0.000\n",
      "Epoch 3/5 - d_loss: 11.291, d_acc: 0.500, g_loss: 0.000\n",
      "Epoch 4/5 - d_loss: 17.393, d_acc: 0.500, g_loss: 0.000\n",
      "Epoch 5/5 - d_loss: 21.594, d_acc: 0.500, g_loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "import csv\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def preprocess_data(data_path):\n",
    "  # Load the raw data\n",
    "  raw_data = []\n",
    "  with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "      raw_data.append(row[0])  # Assume that the data is in the first column of the CSV\n",
    "  # Tokenize the data to convert it into numerical form\n",
    "  tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "  tokenizer.fit_on_texts(raw_data)\n",
    "  data = tokenizer.texts_to_sequences(raw_data)\n",
    "  # Pad the data to ensure that all sequences have the same length\n",
    "  data = tf.keras.preprocessing.sequence.pad_sequences(data)\n",
    "  # Shuffle the data to mix up the examples\n",
    "  np.random.shuffle(data)\n",
    "#   print(data)\n",
    "  return data, tokenizer\n",
    "\n",
    "\n",
    "# Set the input and output dimensions for the GAN\n",
    "input_dim = 100  # The number of words in the input sentence\n",
    "output_dim = 100  # The number of words in the output sentence\n",
    "\n",
    "# Build the generator model\n",
    "generator = tf.keras.Sequential()\n",
    "generator.add(tf.keras.layers.InputLayer(input_shape=(input_dim,)))\n",
    "generator.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
    "generator.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
    "generator.add(tf.keras.layers.Dense(units=2048, activation='relu'))\n",
    "generator.add(tf.keras.layers.Dense(units=4096, activation='relu'))\n",
    "generator.add(tf.keras.layers.Dense(units=output_dim, activation='softmax'))\n",
    "\n",
    "# Build the discriminator model\n",
    "discriminator = tf.keras.Sequential()\n",
    "discriminator.add(tf.keras.layers.InputLayer(input_shape=(output_dim,)))\n",
    "discriminator.add(tf.keras.layers.Dense(units=4098, activation='relu'))\n",
    "discriminator.add(tf.keras.layers.Dense(units=2048, activation='relu'))\n",
    "discriminator.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
    "discriminator.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
    "discriminator.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the models\n",
    "generator.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Build the GAN model by stacking the generator and discriminator\n",
    "gan = tf.keras.Sequential()\n",
    "gan.add(generator)\n",
    "gan.add(discriminator)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Set the number of epochs and batch size for training\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Load and preprocess the data\n",
    "data_path = './data.csv'  # Replace with the file path to the dataset\n",
    "data, tokenizer = preprocess_data(data_path)\n",
    "\n",
    "# Define a function to generate a batch of synthetic data\n",
    "def generate_synthetic_data(batch_size):\n",
    "  synthetic_data = []\n",
    "  for i in range(batch_size):\n",
    "    # Generate a random input sentence\n",
    "    input_sentence = np.reshape(np.random.randint(low=0, high=input_dim, size=(input_dim,)), (1, -1))\n",
    "#     print(input_sentence, \",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\")\n",
    "    # Use the generator to generate a synthetic output sentence\n",
    "    synthetic_output = generator.predict(input_sentence)\n",
    "    synthetic_data.append((input_sentence, synthetic_output))\n",
    "  return synthetic_data\n",
    "\n",
    "# Train the GAN\n",
    "for epoch in range(epochs):\n",
    "  # Generate a batch of synthetic data\n",
    "  synthetic_data = generate_synthetic_data(batch_size)\n",
    "  input_sentences, synthetic_outputs = zip(*synthetic_data)\n",
    "  # Generate a batch of real data\n",
    "  real_data, _ = preprocess_data(data_path)\n",
    "  real_data = real_data[epoch*batch_size:(epoch+1)*batch_size]\n",
    "\n",
    "  real_outputs = real_data[:, -output_dim:]\n",
    "  # Train the discriminator on the synthetic and real data\n",
    "  discriminator.trainable = True\n",
    "  d_loss_real = discriminator.train_on_batch(real_outputs, np.ones((batch_size, 1)))\n",
    "  d_loss_synthetic = discriminator.train_on_batch(np.reshape(synthetic_outputs, (32,-1)), np.zeros((batch_size, 1)))\n",
    "  d_loss = 0.5 * (d_loss_real[0] + d_loss_synthetic[0])\n",
    "  d_acc = 0.5 * (d_loss_real[1] + d_loss_synthetic[1])\n",
    "  # Train the generator\n",
    "  discriminator.trainable = False\n",
    "  g_loss = gan.train_on_batch(np.reshape(input_sentences, (32,-1)), np.ones((batch_size, 1)))\n",
    "  # Print the losses and accuracies\n",
    "  print(f'Epoch {epoch+1}/{epochs} - d_loss: {d_loss:.3f}, d_acc: {d_acc:.3f}, g_loss: {g_loss:.3f}')\n",
    "\n",
    "# Save the trained models\n",
    "generator.save('generator.h5')\n",
    "discriminator.save('discriminator.h5')\n",
    "\n",
    "# Test the GAN\n",
    "def test_gan(sentence):\n",
    "  # Preprocess the input sentence\n",
    "  _, tokenizer = preprocess_data(data_path)\n",
    "  sentence = tokenizer.texts_to_sequences([sentence])\n",
    "\n",
    "  sentence = tf.keras.preprocessing.sequence.pad_sequences(sentence, maxlen=input_dim)\n",
    "  # Use the generator to generate a synthetic output sentence\n",
    "  output = generator.predict(sentence)\n",
    "  print(output, \"first\")\n",
    "#   print(int(np.argmax(output, axis=-1)), \"secnof\")\n",
    "  # Convert the output back to text\n",
    "  output = tokenizer.sequences_to_texts(int(np.argmax(output, axis=-1)[0]))\n",
    "#   output = tokenizer.sequences_to_texts(np.reshape(output)\n",
    "  return output\n",
    "\n",
    "# Test the GAN on a few example sentences\n",
    "# sentences = ['This is a test sentence.', 'I am just trying out this GAN.', 'I hope it works well!']\n",
    "# outputs = [test_gan(sentence) for sentence in sentences]\n",
    "\n",
    "# # Print the results\n",
    "# for i, (sentence, output) in enumerate(zip(sentences, outputs)):\n",
    "#   print(f'Example {i+1}:')\n",
    "#   print(f'Input: {sentence}')\n",
    "#   print(f'Output: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2a866bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]] first\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-942d6c7bfd9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'This is a test sentence.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'I am just trying out this GAN.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'I hope it works well!'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest_gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Print the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(test_gan(\"Great course\"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-116-942d6c7bfd9a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'This is a test sentence.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'I am just trying out this GAN.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'I hope it works well!'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest_gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Print the results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(test_gan(\"Great course\"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-115-740bbb93ff66>\u001b[0m in \u001b[0;36mtest_gan\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;31m#   print(int(np.argmax(output, axis=-1)), \"secnof\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m   \u001b[1;31m# Convert the output back to text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m   \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;31m#   output = tokenizer.sequences_to_texts(np.reshape(output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36msequences_to_texts\u001b[1;34m(self, sequences)\u001b[0m\n\u001b[0;32m    336\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtexts\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \"\"\"\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequences_to_texts_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msequences_to_texts_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36msequences_to_texts_generator\u001b[1;34m(self, sequences)\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[0mnum_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0moov_token_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moov_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m             \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "sentences = ['This is a test sentence.', 'I am just trying out this GAN.', 'I hope it works well!']\n",
    "outputs = [test_gan(sentence) for sentence in sentences]\n",
    "\n",
    "# Print the results\n",
    "# print(test_gan(\"Great course\"))\n",
    "\n",
    "for i, (sentence, output) in enumerate(zip(sentences, outputs)):\n",
    "  print(f'Example {i+1}:')\n",
    "  print(f'Input: {sentence}')\n",
    "  print(f'Output: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62180713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./data.csv\")\n",
    "test = list(data['reviews'])\n",
    "print(test_gan(test[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75509f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
