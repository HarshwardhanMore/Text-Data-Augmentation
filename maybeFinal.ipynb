{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2026dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c15b53ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3c1d871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[data.sentiment == 0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20c13051",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop(['sentiment'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bd1cf416",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81970</th>\n",
       "      <td>Probably the best financial market course on c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81971</th>\n",
       "      <td>From a knowledge perspective, I have learned a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81972</th>\n",
       "      <td>Some parts of the course were informative.  So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81973</th>\n",
       "      <td>The course is not that challenging, it could b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81974</th>\n",
       "      <td>It's more of a course for the people who have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews\n",
       "81970  Probably the best financial market course on c...\n",
       "81971  From a knowledge perspective, I have learned a...\n",
       "81972  Some parts of the course were informative.  So...\n",
       "81973  The course is not that challenging, it could b...\n",
       "81974  It's more of a course for the people who have ..."
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1ec729cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "sentences = list(data_train[\"reviews\"])\n",
    "for sen in sentences:\n",
    "    x.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7f4527a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "vocab_size=10000\n",
    "# Tokenize the text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences = tokenizer.texts_to_sequences(x)\n",
    "max_length=100\n",
    "\n",
    "# Pad the sequences to the same length\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "64114b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 80,   2, 179, ...,   0,   0,   0],\n",
       "       [ 84,   4,  68, ...,   0,   0,   0],\n",
       "       [ 37,  87,   3, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  9,  42,   5, ...,   0,   0,   0],\n",
       "       [  9, 272,   7, ...,   0,   0,   0],\n",
       "       [ 40,  18, 129, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "471e4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model\n",
    "def make_generator_model():\n",
    "    embedding_dim=100\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "    model.add(tf.keras.layers.Reshape((max_length, embedding_dim)))\n",
    "    model.add(tf.keras.layers.LSTM(units=128, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(units=128, return_sequences=True))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size, activation=\"softmax\")))\n",
    "    return model\n",
    "\n",
    "# Define the discriminator model\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    embedding_dim=100\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "    model.add(tf.keras.layers.Reshape((max_length, embedding_dim)))\n",
    "    model.add(tf.keras.layers.LSTM(units=128, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(units=128, return_sequences=True))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation=\"sigmoid\")))\n",
    "    return model\n",
    "\n",
    "\n",
    "# Instantiate the generator and discriminator models\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "# Define the loss functions and optimizers\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "generator_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "batch_size=10\n",
    "noise_dim=100\n",
    "\n",
    "# Define the training loop\n",
    "@tf.function\n",
    "def train_step(real_sequences):\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_sequences = generator(noise, training=True)\n",
    "\n",
    "        real_loss = cross_entropy(tf.ones_like(discriminator(real_sequences, training=True)), discriminator(real_sequences, training=True))\n",
    "        fake_loss = cross_entropy(tf.zeros_like(discriminator(generated_sequences, training=True)), discriminator(generated_sequences, training=True))\n",
    "        disc_loss = real_loss + fake_loss\n",
    "\n",
    "        gen_loss = generator_loss(tf.ones_like(discriminator(generated_sequences, training=True)), generated_sequences)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "# Define the training loop\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in dataset:\n",
    "            train_step(batch)\n",
    "\n",
    "        print(f\"Time for epoch {epoch+1} is {time.time()-start} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0c9ac53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_39_input'), name='embedding_39_input', description=\"created by layer 'embedding_39_input'\"), but it was called on an input with incompatible shape (10, 100, 10000).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"<ipython-input-104-cb5b9f3bdc5c>\", line 45, in train_step  *\n        fake_loss = cross_entropy(tf.zeros_like(discriminator(generated_sequences, training=True)), discriminator(generated_sequences, training=True))\n    File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"reshape_27\" (type Reshape).\n    \n    Cannot reshape a tensor with 1000000000 elements to shape [10,100,100] (100000 elements) for '{{node sequential_40/reshape_27/Reshape_2}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](sequential_40/embedding_39/embedding_lookup_2/Identity_1, sequential_40/reshape_27/Reshape_2/shape)' with input shapes: [10,100,10000,100], [3] and with input tensors computed as partial shapes: input[1] = [10,100,100].\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(10, 100, 10000, 100), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-50ef00681123>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-104-cb5b9f3bdc5c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Time for epoch {epoch+1} is {time.time()-start} sec\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-104-cb5b9f3bdc5c>\", line 45, in train_step  *\n        fake_loss = cross_entropy(tf.zeros_like(discriminator(generated_sequences, training=True)), discriminator(generated_sequences, training=True))\n    File \"C:\\Users\\HP\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"reshape_27\" (type Reshape).\n    \n    Cannot reshape a tensor with 1000000000 elements to shape [10,100,100] (100000 elements) for '{{node sequential_40/reshape_27/Reshape_2}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](sequential_40/embedding_39/embedding_lookup_2/Identity_1, sequential_40/reshape_27/Reshape_2/shape)' with input shapes: [10,100,10000,100], [3] and with input tensors computed as partial shapes: input[1] = [10,100,100].\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(10, 100, 10000, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(padded_sequences)\n",
    "dataset = dataset.shuffle(buffer_size=len(padded_sequences)).batch(100)\n",
    "train(dataset, 10)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f21c12a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cdf1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
