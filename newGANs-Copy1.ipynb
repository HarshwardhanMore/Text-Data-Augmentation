{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c73ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.layers import GlobalMaxPool1D, Conv1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4ae130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed51e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6e8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.sentiment == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205d99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(sen):\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2209640",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd799c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "sentences = list(data[\"reviews\"])\n",
    "for sen in sentences:\n",
    "    x.append(preprocess_text(sen))\n",
    "\n",
    "y = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd1317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70e3f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)    # This will limit the maximum number of words to 10000\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100      # Maximum length of sequence is capped at 100\n",
    "\n",
    "\n",
    "# Padding the sequences to match the length\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "x = pad_sequences(x, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35cf2e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cecbf2bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_10/embeddings:0', 'lstm_10/lstm_cell_10/kernel:0', 'lstm_10/lstm_cell_10/recurrent_kernel:0', 'lstm_10/lstm_cell_10/bias:0', 'dense_14/kernel:0', 'dense_14/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_10/embeddings:0', 'lstm_10/lstm_cell_10/kernel:0', 'lstm_10/lstm_cell_10/recurrent_kernel:0', 'lstm_10/lstm_cell_10/bias:0', 'dense_14/kernel:0', 'dense_14/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "1359/1359 [==============================] - 1213s 892ms/step - loss: 0.0028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a03467fa0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Set the dimensions of the input and output data\n",
    "input_dim = 10000\n",
    "output_dim = 1000\n",
    "\n",
    "# Define the generator model\n",
    "generator = tf.keras.Sequential()\n",
    "generator.add(tf.keras.layers.Embedding(input_dim=input_dim, output_dim=128))\n",
    "generator.add(tf.keras.layers.LSTM(128))\n",
    "generator.add(tf.keras.layers.Dense(output_dim, activation='softmax'))\n",
    "\n",
    "# Define the discriminator model\n",
    "discriminator = tf.keras.Sequential()\n",
    "discriminator.add(tf.keras.layers.Embedding(input_dim=output_dim, output_dim=128))\n",
    "discriminator.add(tf.keras.layers.LSTM(128))\n",
    "discriminator.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Define the GAN model\n",
    "gan = tf.keras.Sequential()\n",
    "gan.add(generator)\n",
    "gan.add(discriminator)\n",
    "\n",
    "# Compile the GAN model\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the GAN model on your dataset\n",
    "gan.fit(x_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ef24a70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.135518e-06]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tf.random.normal(shape=(1,10), mean=50, stddev=10)\n",
    "# x = tf.reshape(x_test[0], shape=(1, input_dim))\n",
    "# Use the GAN to generate new samples of data\n",
    "generated_data = gan.predict(z)\n",
    "\n",
    "# Save the generated samples to a file\n",
    "# with open('generated_data.txt', 'w') as f:\n",
    "#   for sample in generated_data:\n",
    "#     f.write(sample + '\\n')\n",
    "\n",
    "generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10dc41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3c2d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array(z).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f4e771e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48, 62, 50, 47, 39, 42, 64, 60, 43, 61]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ebf8602",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.reshape(z, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "068983c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 62, 50, 47, 39, 42, 64, 60, 43, 61])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "566bf47a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no great should can so me week one i learning']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8da441c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for i in range(10000):\n",
    "    z = tf.random.normal(shape=(1,10), mean=5000, stddev=1000)\n",
    "    z = np.array(z).astype(int)\n",
    "    z = np.reshape(z, 10)\n",
    "    new_data.append(tokenizer.sequences_to_texts([z]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8982657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be4462fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_append = []\n",
    "for i in new_data:\n",
    "    data_to_append.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e286b402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tossed resultados routing downhill axis narration dito elevado principals letting',\n",
       " 'region apt crashes flight transmitir leadership writers grey hack female',\n",
       " 'specifics redacci aburrido rooms desarrollar capacity supports innovative canadian eliminate',\n",
       " 'weaknesses dissapointed ca didactics marker viewed ellas advances erroneous llamar',\n",
       " 'yaakov qualidade fader asi conocer figures artistic hora genetics assn',\n",
       " 'reduces acess revisions recommened claim theoritical auditor instagram switching hall',\n",
       " 'ps bummer upgrading beautifully behavioural fleshed lawyer participaci inevitably mes',\n",
       " 'cuestionario cross composed audiences raise guessed ao union contenu citing',\n",
       " 'manually backwards combinations prove manufacturing problema physics indepth ensemble filmed',\n",
       " 'sees operate proud asset libro flying anza executives programar graduated',\n",
       " 'arent wird entirety suit skilled resolving authority bite ayudar reinforcement',\n",
       " 'private navigating lays ingles color ein intonation perd chemical visited',\n",
       " 'diseases courser justification coders ceased begins dev distinguish deserved datos',\n",
       " 'segunda unbelievably preparaci began losing formulate entendible intellectually canvas deserved',\n",
       " 'nuevamente walkthroughs occasions pe tad perception totalidad estimations dsx reduces',\n",
       " 'acquiring failure titles fica questionable unbelievably confusas pondre ups exemples',\n",
       " 'photographers estas mental sending lovely solvable bubble digested reasoning addressing',\n",
       " 'processo ly cohesive peculiarities remarkable callback wolves jumbled opening parecio',\n",
       " 'v compute indigenous likes tab replied vedio complementario assure apple',\n",
       " 'marketplace muestra fin paz pedag coach resolving sejnowski honor supportive',\n",
       " 'equipped promised clustering interviewing holding glitchy photographers ar meals faculties',\n",
       " 'replication traducciones mentality espec webcasts ruined presentados avoir lucid temario',\n",
       " 'pudieran shots semantics mejora mouse alternate wasnt waston strokes stone',\n",
       " 'rewind grasping veterinarian whatnot revisited folders companion newly exploring dislike',\n",
       " 'heck benefitted photos surprised mixing reproduce scarce properties justification abstracted',\n",
       " 'hablando participating mentality breve tutoriales bubble mantener customers asap act',\n",
       " 'bag manufacturing ac cat loose inserted joke listing quelques jupytar',\n",
       " 'immense decades begging senior poner altered addiction perd plate comprehensible',\n",
       " 'vastly iot servers gas regulations resulted profissionais rushes tad passable',\n",
       " 'anxiety iterations georgia quotes convention micas revised honor cumbersome complains',\n",
       " 'differ resultado programmatic mixture joined cc played workbook cache succinct',\n",
       " 'propose explicitly purchased fuzzy asd maps express redacci gen relatora',\n",
       " 'fluffy quest ainda implies ironed calculator challenged combined educator crazy',\n",
       " 'pauses negocios extensions frauds incluir dissapointed algumas thumbs earth happens',\n",
       " 'persons chase positives indepth automation blue ponen institute bare recommender',\n",
       " 'descargar frequentist merged zanzibar revoir uni burned foo picks pide',\n",
       " 'fooled connections matching relaxed remembered pregunta remarks explican string infra',\n",
       " 'pursuing foram union fond uniform chemical mcqs recently unsure emotional',\n",
       " 'practitioner joke expectativas pitched investors frustrations equipped remarkable academia qualify',\n",
       " 'ba mainstream saving styles desperately privacy typically excelentes arts character',\n",
       " 'typically painfully subjected seligman iterative scheme nimo shooting ambiguas ought',\n",
       " 'strings disheartening pool tracks determining genes remembered row former klout',\n",
       " 'masters threw divide ben reports evidenced tag adquirir temps ein',\n",
       " 'launch streamlined inclusive aprendidos manipulate ayudan audiovisual sped dognition brinda',\n",
       " 'puedan necesita puns scroll merge incluir captivating pocos connections scratches',\n",
       " 'classroom suit consultas vpns diplomatic mentality avoiding extenso absent pobre',\n",
       " 'vectorized bag supports uniform hardest nutritional convolutional cm lighter seguridad',\n",
       " 'monitor separated guiding center comandos yoga uc melhorar float tampoco',\n",
       " 'stands empathy developments jumbled proceed debate tho connections toda academics',\n",
       " 'entendible phrase muita alguno scroll smoothly shifts nasty academically allows',\n",
       " 'lengua akin edx inaudible spoiler navigating traducci conceitos critic dulos',\n",
       " 'hall nas terminar dito capacitaci parents necesarios picks modifying usar',\n",
       " 'specializations complicate resubmit graduated commended spaced assure gotten scipy disease',\n",
       " 'peek overlapping bite cv entregar parecido pocos universe dataframes pandemic',\n",
       " 'component department frames prevent equipped unenrolled lectured uniform voire factors',\n",
       " 'objetivo zoom mejora incomprehensible additions physics agradezco escrito assesments outright',\n",
       " 'criticisms british lively pivot hasn gripe quisiera pls shares distinguish',\n",
       " 'enhanced companion squares immense confortable hac fond akin margin entrepreneurship',\n",
       " 'scipy dump speeds operational schemes rhythm niveau micro lengthen ordinary',\n",
       " 'moma existe profesionales signature quest accompany scraping acess aspiring hte',\n",
       " 'mouse practising diets highlights solver mouse sustaining integral advices stopping',\n",
       " 'periods ace mencionan disclaimer blind classical profesora ruim recognized beaucoup',\n",
       " 'safe kick unknown systematically ratos poorer visit permet nuggets postings',\n",
       " 'realized promised materia sima contribute yoga compile browsing filling tendr',\n",
       " 'callbacks empathy klout clumsy begins illustrator schema principal figures trop',\n",
       " 'wild dulos nick backing conclude unsupervised cheesy frequent easiest drugs',\n",
       " 'acerca intent god proceeding revisi spinning norm inspiration intervention manufacturing',\n",
       " 'principle null explicativos powerpoint sustaining magic profundiza paused semantics meditation',\n",
       " 'owns compile nasty vedios simulator humble presumably depended consistently webpage',\n",
       " 'tenido root poner fund mastered adopted thumb thesis revisited allocated',\n",
       " 'fundamentally manejo profesionales radio cotejo describing fuel portal confusions crappy',\n",
       " 'salt filter gratis pedag subpar wow availability codeacademy overpriced arrays',\n",
       " 'exploratory nimo converted lista attract forecasting unengaging safari anchor tte',\n",
       " 'confidently tait originally similarities graduation reciting profesora modo cleaner repo',\n",
       " 'lag apple workings pregunta financing delves transcriptions ne estoy plz',\n",
       " 'contradicting pedantic plagiarized textual assertions categories werent maximize charged muddled',\n",
       " 'inactive domains unchallenging arrays avoiding pratical abbreviations penalized locations unsure',\n",
       " 'enriching contable inter enriching applicability lengua geometric fluidez routing callback',\n",
       " 'inclusive excitement sdgs replicate interpretations praised dificil medida folders provoking',\n",
       " 'cache sale node refinement category collect counted discrepancies establishing instruments',\n",
       " 'georgia addiction disclaimer keys western differentiation acting qualities exclusively credentials',\n",
       " 'graduate justification dollar academics eclipse proposal excessive rios presumably challenged',\n",
       " 'yellow shares callbacks ruined nutella practicals sweet ball teams expectativas',\n",
       " 'claims shooting golang mir cheers disappeared embora unfocused urllib hashtag',\n",
       " 'official coherent genomics canadian bot responded realized menus guessed visualisations',\n",
       " 'partly chemical logically paused aplicando robot increasing inference teams iterations',\n",
       " 'cheers monitor di participating hacerlo principal relation presumably pitched emphasize',\n",
       " 'sending documentary describe applies affects mixing scrap vary questionnaires indepth',\n",
       " 'regulations cs viendo orienting outlining minutos puesto webcasts contrary troubleshoot',\n",
       " 'parsing tutors unlock responded filtering empathy abbreviations wild curators newcomer',\n",
       " 'exceeded precisely fitting cert communities justify chemical cap performing begineers',\n",
       " 'idioma visualizar birds blog comprehensible turns meal unsupervised gamification spectroscopy',\n",
       " 'gina rolled modifying alta authority estoy picks corresponde firm seligman',\n",
       " 'panel renewable necesita occurred lying newly percentage pack incorporates opposite',\n",
       " 'fruit mechanics shortcoming seg simplest essence macbook dates intentionally stands',\n",
       " 'dire manipulating intent summarizing fora recommendable salt brainstorming delivers depended',\n",
       " 'dato sans transcriptions blue supplemented unknown vastly acredito cheating caveat',\n",
       " 'instrument enterprise complementarias contributed childish misunderstood annotations pythonic viewed arrived',\n",
       " 'moves importing impactful dataframes estan nowadays startup communities speciality pandemic',\n",
       " 'pedagogical dollar handbook novices sobretodo act x drawing formation systematic',\n",
       " 'played sas necessity mathematic estrategias skims hastily misspellings warn incorporate',\n",
       " 'halfway apt faint aways recruiting assimilate pytorch demotivated tongue claims',\n",
       " 'plagiarized applaud encargados tenia passive intuitively cierto disheartening approfondir paywall',\n",
       " 'schemes arroyo puntos translations laboratory meal observations quedaron moma attended',\n",
       " 'grande handful altogether investigaci gdp codecademy boost remembered tidbits strokes',\n",
       " 'men backing muita simo apples titulo mejores scoring percent photographs',\n",
       " 'referencing delays neat affects impart ba profitable easiest lighter conocer',\n",
       " 'tive visualisation apples kotlin succinct shall st davis contrary demonstrating',\n",
       " 'tout transparent bag handed outlining atom backend grey marker disrespectful',\n",
       " 'welcome canada lowest classified conducive decrease banking necesidad crappy mcqs',\n",
       " 'ven upcoming suspect encompasses specification surely root draft severe boost',\n",
       " 'materia mencionan array diverse complejo cleansing espec likable dropping gce',\n",
       " 'coders closed nns funciona definitivamente fluid perceived tulos ppt reas',\n",
       " 'trough archive supposedly thin stellar workflow utile confortable scaffolding meu',\n",
       " 'gripe mir exemples victims delay wolves melhor inclusive redone listeners',\n",
       " 'apuntes severance webcasts pycharm span theoritical berger fee tenia repository',\n",
       " 'specializations pandemic involves kahn wordy log labeled air ignoring escuchar',\n",
       " 'emphasizes nasty pretrained startup advances baseline collaborative importantes contribute grande',\n",
       " 'unos macos wrongly rational construct reduces ont export apps organizing',\n",
       " 'scratching discovered bem bird tongue convert dislike reaching evident aspecto',\n",
       " 'demonstrating accompanied mediocre string citations dollars helpfull toughest ex minus',\n",
       " 'seemingly assure understands brushed addiction packed discusses datalab outlined landscape',\n",
       " 'findings budding switching iceberg hare squeezed tico century interpreted tienes',\n",
       " 'explicada scoping trade portfolio scarce jason corona earth bugged fo',\n",
       " 'panorama hacker carrying complejo redes suit actualizado recommending recruitment leyendo',\n",
       " 'inicio repaso sdgs failure palabras boost takeaway consists achei verified',\n",
       " 'sas hrs permissions completamente administrators universidad subtleties delivering unhealthy scoring',\n",
       " 'strengths cr sale owner servers globe inference dairy router behavioral',\n",
       " 'faulty excessively modify channel assunto misplaced ignores atenci gods solely',\n",
       " 'salt fighting ages ridiculously spirit genuine assigment pratical theoritical citations',\n",
       " 'pool water profesional wird tiring soviet accepting phrased forecasting lifestyle',\n",
       " 'worry wrangling computational manipulating aids humour johns handful disparity ingredients',\n",
       " 'versi fell hammering decipher scrapping lets pessoas fraction macros manageable',\n",
       " 'anterior vegetarian toda predict advantages income cairo occasions waited locally',\n",
       " 'march cheers clueless complet companion strict vary robots kernel empty',\n",
       " 'legit largo acting dificultad involving exploratory empieza shapes operational tag',\n",
       " 'dumbed damping massive stands pollan locked practicals badge shorten framing',\n",
       " 'wall lets recomendar uc shiller di illustrating awkwardly sas atom',\n",
       " 'colors sehr selected replication kill gustar adjusted conduite presentados blocked',\n",
       " 'tico repetetive pathetic revisions libro hashtag peace thereby discusses desarrollo',\n",
       " 'occur wrongly claras flash grande nutricional notations everywhere credentials deveria',\n",
       " 'relaci intructor decks educated plataforma grip microsoft electrical stream tuition',\n",
       " 'sooooo und classic misled exp bite brought experimental nutricional unenrolled',\n",
       " 'evening polynomial oriques ci ven construct adecuada confuso broaden framing',\n",
       " 'android hammering selecting reflection drag adjust cat hype institutions sdgs',\n",
       " 'interpretation triplet apps beverage photos summarised lightweight segunda unstable macbook',\n",
       " 'gina puedes zoom decrease determining arbitration exp esperar perception hashtag',\n",
       " 'profundiza proceso unfocused experi sobretodo elas embarrassing evolution individually healing',\n",
       " 'uk quiklabs ramps regulations er anytime discrepancies contributed bulleted deveria',\n",
       " 'proficiency drastically entendiendo immense fuentes spinnaker sugar cache adults regulations',\n",
       " 'storytelling actionable globe began behaviour urllib esperar passes printing welcomed',\n",
       " 'instructores realidad conceptually philosophers remotely finals channel collaborate unfinished monologue',\n",
       " 'negocio peque merge folders fluid consists pouco saving parallel entrepreneurs',\n",
       " 'delays stackflow distance consume purchasing manipulating pratique l individuals yellow',\n",
       " 'trop usuarios intent unavailable dashboard delays hyperparameter thru timeline imparten',\n",
       " 'blind predictions especial estas cr totalmente desarrollar progression recruitment render',\n",
       " 'stands located viendo webpage theoretic act dozens efficiency agency assertions',\n",
       " 'upcoming consultas convnn scipy apps beware ao drawbacks intent bash',\n",
       " 'impractical cousera fala square dependencies fragmented quest enhancing quicklabs theoretic',\n",
       " 'participant extensions clumsy sharp british executives recipe contributions preferably endless',\n",
       " 'dislike validation flash usages bridge veo helpfull analogy vary marker',\n",
       " 'begins comma hablar labelled incognito dictionaries gut lengthen capacity comer',\n",
       " 'inspired comer themes letras encouragement def breeze tidyverse react acerca',\n",
       " 'genuinely semantics startup acces instrucciones davis concretely accepted hopkins virtually',\n",
       " 'repo restart akin orienting mencionan qa dispon equity strictly brochure',\n",
       " 'conceptually operational prevalent category eval rushes unavailable iterations colours amateur',\n",
       " 'merge location facial dificil encompasses trick fruit combined layman lugar',\n",
       " 'mcq disorganised evolution gratis expanding experiencia water complejo profession overpriced',\n",
       " 'weaknesses fruit fundamentally composed tempo cient communities gripe defining designer',\n",
       " 'existent film adecuado ambitious textual exponer dived replacement praised preferably',\n",
       " 'triple impart anticipate quicklabs reste geometric region delayed stochastic ish',\n",
       " 'downer sensitivity attacks hack reduces parecio gdp sobretodo hair communities',\n",
       " 'preparaci financials famous michel onwards panda knn stretched obtain limit',\n",
       " 'automatically claims sans innovative associated sans encounter entiende emulate hora',\n",
       " 'ingles structural travel systematically streaming monkey evaluado frustrations unhappy convolution',\n",
       " 'capacitaci viz sendo melhorar ambigua separator trees backgrounds controller controversial',\n",
       " 'purchased efficiently ninguna loose enfoque quicklabs trough biological forget comienzo',\n",
       " 'correr restarted garbage overload harm handful drawing tedioso shots specialized',\n",
       " 'pet nuevamente complementar conveys ein regret chaotic explored prewritten joins',\n",
       " 'simulator evolution throwing exemples sooooo assesments manifesto cet gratis container',\n",
       " 'servers abarca stopping ethics entiendo specialized organisation gladly cheat classic',\n",
       " 'rios overlapping prueba channel counting cc whatnot trick impart var',\n",
       " 'perception suggesting returned cheers asd abordar quem utterly lesser visto',\n",
       " 'concentrated colocar lawyer compensate cleaning pagado critically solvable empresa iceberg',\n",
       " 'obtained attitude loaded fortunately government excitement bucket deployment accordingly members',\n",
       " 'primeros budding contribution clearing monthly squares devoted standpoint hyperparameters stopped',\n",
       " 'requisite demonstrating ayudar egypt logically complejo mba ejercicio bigquery employees',\n",
       " 'apes analyzing deepen nns associated remotely raised strokes playback pun',\n",
       " 'extenso rooms pocos concret nns recordings overlapping monte perception apples',\n",
       " 'habit filosof expose employed mismatch assn docentes sounded titles incluso',\n",
       " 'meanwhile conversational success india court sale txt seaborn lovely aids',\n",
       " 'bridge label frustrante pacing sized accomplishment stone diploma nao grow',\n",
       " 'inconvenient warrant entienden urban printing standpoint concisely lesser modulo alpha',\n",
       " 'pedantic candidate relaci alguno pouco neatly redone trough alguna melhorar',\n",
       " 'launched diaria uk gratis stupid peut delves solidify viz criticisms',\n",
       " 'extensions dia lengua invested consumer documentations silly pude air lag',\n",
       " 'letras reflects wholly official letters loop forecasting proposed scraping air',\n",
       " 'foot bocconi nica regressions grammatical exemplars micro agregar usuarios recommender',\n",
       " 'patterns arent roughly emotional leaders periods race scratching namely relatora',\n",
       " 'vegan squeeze unexpected ejercicio polish medida ft illustrating tenia avoiding',\n",
       " 'loads recommending brushed quietly doctor fundamentales contadores libro knowledges microsoft',\n",
       " 'existed associate melhor interruptions addressing road gen entienden playground ensuring',\n",
       " 'aplicaciones house mathematic administration array operator designers moderate redesign awkwardly',\n",
       " 'preferably tv descent intellectual ninguna sucks famous tarea sincerely sehr',\n",
       " 'endless revisar entiende wrangling considerations ademas refinement ft niveau excersice',\n",
       " 'overloaded targets unhappy attitude transcription situational vegetarian polish disparity aulas',\n",
       " 'accessibility validity interpretations famous troublesome boy complementar launch hacerse guessing',\n",
       " 'mill returned egger forecasting disclaimer observations explicaron hated correcto variation',\n",
       " 'authority investigaci skims ses node berger visualisation responsible comparisons tab',\n",
       " 'compression ampliar experimenting restricted audiences proceeding strangely string outlines ensemble',\n",
       " 'lies timelines definitivamente precisely gb repeatedly toma extension alguna sujet',\n",
       " 'mill translator marks terrific ciertas tbh minha unstructured cop throws',\n",
       " 'crisp correlations egypt outils winded space canadian specificity disclaimer soporte',\n",
       " 'articulate critic somos compilation reminders idiot curse tenido joins originally',\n",
       " 'exponentially accepting justified brinda centred repetitions virtualbox justification seemingly egypt',\n",
       " 'blue expectativas overfitting notations fourth posible pauses explicativos pushed thousands',\n",
       " 'oddly count semantics backend pool analyzing upcoming bigquery availability fixes',\n",
       " 'dirty gamification embarrassing gender environmental document interventions puzzle resultados tracks',\n",
       " 'allotted cryptography woth rooms intensive relation verbose sustaining carry vectorization',\n",
       " 'carbs improves uniform webcasts viewpoint eliminate computational emphasizes teachings cap',\n",
       " 'juniors evening fallacy varying quiklabs browsing undergrad efectivo pseudocode zu',\n",
       " 'grace acquire verbally center childish joining datos arent begining proved',\n",
       " 'sports judging synced persons packs sincerely assingment wishing brushed capable',\n",
       " 'pushed captured ware utilizar race ainda aways acerca ratios strict',\n",
       " 'modulos tuple editor robot differentiate correlations colleague compute muita arithmetics',\n",
       " 'schema flawed transcription wastes climate orienting retained antes proficiency quedaron',\n",
       " 'depende decisiones hammering broadly explicados practico cheaper aussi portfolio ltimas',\n",
       " 'operator outlook replicate consists addressing misunderstood coders ambigua segmentation conversational',\n",
       " 'availability util labeled remembered medio shares sejnowski flawed benford outlines',\n",
       " 'monkey ampliar harry weights debido restarted hyperparameter interacci optimal proud',\n",
       " 'seaborn marks conducted bootstrap phases boot personality esoteric rater peque',\n",
       " 'bird strictly sans shut profissionais fueran doc smattering posed antes',\n",
       " 'entendimiento click backpropagation determining operational grow rambling fake strictly heads',\n",
       " 'critic summarizing anticipated inform tablas qwicklabs experimental contagious addressing beginers',\n",
       " 'satisfaction meaningless incredible formas moderately behaviour navigation compiler intonation exercised',\n",
       " 'educator mathematic bees outstanding inspiration doe manipulate trials simulator effectiveness',\n",
       " 'staying variance unsupervised employees comparisons schools additions advantages rl photographs',\n",
       " 'walked juypter seemingly junior actualizaci recommending expired carrying allocated pobres',\n",
       " 'utilizing sincerely georgia syntactical hecho brings maximize recourse proposed young',\n",
       " 'visited criticisms cloudera physical optimal lol dialogue gru investigaci previos',\n",
       " 'handful nao demonstrate fuzzy udio existe comandos backwards kernel textbooks',\n",
       " 'merits unfortunate constructs webpage unhappy clv repiten formed collected rectify',\n",
       " 'variation exclusively absence linking johns mientras vezes robot appealing fala',\n",
       " 'aides decks heart stepik clave validez race soup disparity ramps',\n",
       " 'rutina minus aspecto urllib visualisations comparisons warm bibliography softwares autism',\n",
       " 'pedagogy muita usar experiments faltaron availability resilience rubrics glimpse solamente',\n",
       " 'formulated attend profundiza throws abrupt disease tongue desarrollar alimentos severance',\n",
       " 'golang schema qwiklab accordingly incorporate colors mastering generales broaden snippets',\n",
       " 'organisation energetic nuestros ticas css condensed extreme clave respective reachable',\n",
       " 'bi precisa esos backgrounds removing entendible asd althought induction comentarios',\n",
       " 'printing secuencia entrepreneurship panel foot brushed caches surprising dear modification',\n",
       " 'navigation philosophers adicionales styles categories startups nscht doubtful conduite indices',\n",
       " 'ado intructor constructs deja recognize analysts comprender personales egypt curated',\n",
       " 'scratching warrant keyboard md justification productivity deveria sistema pena carries',\n",
       " 'stiff clients inform struggles implied precision accompanied analyst unengaging partial',\n",
       " 'comprehensible audited hopeful checks usar anyhow colleague complejos manejo shirt',\n",
       " 'verified concret positives upto arroyo intellectually cited primitive shot excels',\n",
       " 'uninspired mathematic quisiera urge amongst trabajar shots substantially worried credits',\n",
       " 'prescriptive perd represents influence mechanical tune laundry contribution ins parecio',\n",
       " 'traducciones commented gaux fear strings node index amplio severely tuples',\n",
       " 'blue apps np configuration plagiarism notations tensor sequences paul communities',\n",
       " 'outright ticas auditory preguntan cv diapositivas vezes corresponde applets semantics',\n",
       " 'pratiques roll bon blind whatsoever conventions relevance dev importing fund',\n",
       " 'carrying thesis administrators comparisons impacts artworks emphasize computador articulate encounter',\n",
       " 'x utilizar prep ubuntu aqu proves trade aprende combinations entirety',\n",
       " 'competent inbuilt desarrollo werent unavailable sujet redundancy license ben polished',\n",
       " 'embeddings disclaimer digos attempted quienes hint mini yale servers aussi',\n",
       " 'respuestas aprende markdown roughly corresponde accompanied claramente gina peoples profesional',\n",
       " 'anticipated affected bi scattered lugar monte comentarios exposed plans diploma',\n",
       " 'transformation asap toes alphabetically reaching importantes fp squeezed atrociously sirvi',\n",
       " 'systematically legitimate cheating multiplication picking memorization demotivating recommender visually feed',\n",
       " 'deducted intervention cc climate convert pide succeed explicaron mother meta',\n",
       " 'trop nutella rl nail ninguna chf translate resilience gods remedial',\n",
       " 'balanced convince dictionaries offline productivity tenga encompasses apache captured situational',\n",
       " 'baseline loosely failure bibliography profs sugar approved nimo war indices',\n",
       " 'codecademy deserves honor achievement funciona kotlin vegetarian centricity leverage ninguna',\n",
       " 'gist entenderlo screenshot ninguna opci demanded prediction kimberley nonsense indicators',\n",
       " 'backpropagation definitivamente segunda discusses omitted llevan robotically schools indentation intros',\n",
       " 'additions distribution theoretic separated assigned sharp dev feet bibliograf informacion',\n",
       " 'guptarahulgupta conclusions introductive folder rminos scaffolding begining poorer organisation representation',\n",
       " 'varying micro financieros delves trend mba restriction kernel kudos dataprep',\n",
       " 'misconceptions stackflow sports contestar sums sobretodo backing sens consist scroll',\n",
       " 'horrendous emphasize traducciones ms bright gu nuevas correcting conclusions replication',\n",
       " 'dipper opaque throwing ampliar conditional coaching dtu simples dealt wolves',\n",
       " 'pleasant kernel proofread ein gulf professores criminal disparity colors proficiency',\n",
       " 'flash damping strengths dito male express collaboration dedication pratiques aclarar',\n",
       " 'reflecting keeping artwork indicators estilo dedication misma urge accepting imagination',\n",
       " 'picks ar email famous modulos lit hollywood boost viz faculties',\n",
       " 'populate loses breve promotional criticisms safe temps scales verified voices',\n",
       " 'comandos aulas pitched blackboard approche ironed enfocado outs melhor tidyverse',\n",
       " 'situational satisfaction exhausting render excels ambiguities participating assistants workbook somos',\n",
       " 'coded tactics ci retake lite pratical aun cela parameter disheartening',\n",
       " 'picking coping urllib monologue facilitator hac ciertas misplaced reuse ingredients',\n",
       " 'trading journal egger arrive reachable fondo veo item generales row',\n",
       " 'macos visi usar proceso cellphone orique pursuing represents badge qcm',\n",
       " 'esquemas pull differs tidbits observation deben revisions house rank march',\n",
       " 'egger verbally urban candidate interspersed correctness tuviera treat niveau strict',\n",
       " 'medicine pf detalles salt hacerlo abilities fuzzy hare acredito feasible',\n",
       " 'canadian desktop consistently evolves wondering lectured disappeared imagen jupyterlab mico',\n",
       " 'tenido landscape replied credential mechanisms skin unlock categorized signing parecen',\n",
       " 'progressively reminder category ensuring ricos intersting happiness threat deja absent',\n",
       " 'participantes vc profs autism unsure parecio net gen happier beautifulsoup',\n",
       " 'bubble act determining igual onboarding descargar engineers discrete ca childish',\n",
       " 'fatigue enormous combinations minutos plataforma stopped emphasizes pulled qualit investigate',\n",
       " 'conversational passive zoom notations lexical shortcomings temario introductive occasional aussi',\n",
       " 'sustaining appropriately afford arrived centred monotonic assignements assignement unhappy michael',\n",
       " 'estructurados penalized assign death trough assn specify sensaci indicators peng',\n",
       " 'roi war niveau ando apuntes odd institutions embeddings interacci rminos',\n",
       " 'estrategias periods picky trials imaging modifying budding fluent centricity wastes',\n",
       " 'motivational installations surprisingly torna ci critics ciertos category opt pseudocode',\n",
       " 'sweet overload demotivating varias huh guideline whilst browser approaching distinguish',\n",
       " 'binomial meditation kurs communities disagree funcionamiento coders audiences formulation pueda',\n",
       " 'definite hey refinement martin operational generar sklearn bloom functionalities lightly',\n",
       " 'sheer trials weaknesses region somos evolution pocos outlined tuples pausing',\n",
       " 'taker digos despu formulation bee shorten amateur blind tout enroll',\n",
       " 'cpu float reinforcement hall toes paused alimentos wich formats index',\n",
       " 'squares x amplia fitting accessibility isso crafted scattered rude relaxed',\n",
       " 'troubles nutella nail illustrated rigour gods city nail cryptography momento',\n",
       " 'recently multivariate fighting rarely sided beating aiming nas fader sem',\n",
       " 'verbatim absoluto simplest takeaway observation instagram inexperienced honor remote exponer',\n",
       " 'schemes institutions anyhow gru vegan dependencies naive edx generalized bummer',\n",
       " 'percent estrellas upcoming toes imaging income reducing obsolete dirigido entendimento',\n",
       " 'armar flexibility firstly film hypothetical impart libro describes facilitator evolves',\n",
       " 'healthcare weakest ft puzzle newly sucks pack builded shares victims',\n",
       " 'terwiesch skims dedicaci kotlin earnings finanzas pedantic overhaul approche textos',\n",
       " 'horse orientado aspecto smoothly similarly enable critic estan mildly sensaci',\n",
       " 'estados reuse hell practicar penalized schemes fashioned llevan unidades curiosity',\n",
       " 'moma bag quarter continuity pet routing proficiency spoke analyst variations',\n",
       " 'constraints awkwardly examen emotions madam regularly retained iterative instrument unforgiving',\n",
       " 'foods apples decks construct ello handled rewind inappropriate tutoriales society',\n",
       " 'theme calificaci claim aspecto quisiera ma tweaking reflecting parec respeto',\n",
       " 'behaviour pulled leadership lets gdp formatted assertions geral tv mathematically',\n",
       " 'prediction sacred ranking disclaimer tbh cloudera ficas solver yellow inconsistency',\n",
       " 'microphone runs fluff sas semi peng conducive curious parec queda',\n",
       " 'hopeful improves taker indentation snippets institute congratulations yellow merging abbreviations',\n",
       " 'facilitator lockdown earnings weights segmentation ridiculously principals alphabetically inexperienced apps',\n",
       " 'dates mere minha dados jupyterlab er tab amazed layman ses',\n",
       " 'empty budding technicalities magic charting leader aclarar sampling pollan adapt',\n",
       " 'decks didactics meu restriction pratical remotely outright ft verbiage banking',\n",
       " 'paragraph simultaneously bees terminal emphasizes nombre aims suspect reviewers payed',\n",
       " 'approachable player clever boredom retention aloud louder netamente apps customers',\n",
       " 'male ptsd lstms pseudocode purchased credential llegar emphasize noticeable brian',\n",
       " 'scipy recession faltan toes groups statistic flying battle variation calculo',\n",
       " 'tenia aburrido overviews ti atlassian shots sagemaker scare fighting valuations',\n",
       " 'drawn meaningless incorporating pandemic letras emerging periods prevent distance entrepreneurship',\n",
       " 'divide tense hack blindly res utilizar deliverable excellently incorporated extensions',\n",
       " 'promoted sans ms originally feasible answering estado split instrument keeping',\n",
       " 'browsing supposedly entrepreneurship meeting convert competition tenido united momentos venture',\n",
       " 'ms justified partir pulled applicability smoother cleaner toes designers extracted',\n",
       " 'principals emotional fica calculated persons inclusive embarrassing coping scrum prs',\n",
       " 'unrealistic private passion ample plz asap sign sick pense deducted',\n",
       " 'assertions eg hte commentary implied candidate reste pretrained relationships bird',\n",
       " 'lento lee visited brinda dumbed muddled integrate factored recommending unbearable',\n",
       " 'preference beautifully calculated applets longest orientation garbage datacamp nuevo remembered',\n",
       " 'fancy presentados vectorized hashtag precision cursory reinforcement replacement throwing combinations',\n",
       " 'breakdown beating executing elas funciona faculties calm cognition modified mess',\n",
       " 'approachable percent terminar zoom kernel cats replicate viral ofrece acquiring',\n",
       " 'encargados mnist aids replicate focussed initiating nuevas matem russian deducted',\n",
       " 'returned dados lucky commonsense cousera assn caliber randomly sobretodo pivot',\n",
       " 'basa spin hrs scratches sehr hte fragments calm margin action',\n",
       " 'tico informa tendr appreciation stepik passion defining hashtag pollan produce',\n",
       " 'flight regressions installed estados unprofessional specifications browsers considerations department manipulate',\n",
       " 'claim factual forgotten colocar mixture datalab np unforgiving nimo nasty',\n",
       " 'dots tenemos controversial ruined standing spoiler excellently claim drag accepting',\n",
       " 'mastered revisi prewritten bloom unintelligible mentality irritated additions imagen emotion',\n",
       " 'actors behaviour pitty respectful reworked cr tie aprobar competition repiten',\n",
       " 'utilizan buenos removing diagram manifesto neo trough universe integrate hence',\n",
       " 'outlook terminar protocols rewind precisely seemingly disclaimer amplio artistic oddly',\n",
       " 'manifesto clearing owns contradictory motivates hung kudos distance avoiding phyton',\n",
       " 'gaining breakdown water nimo predominantly creatively desperately lightweight cumple executives',\n",
       " 'squares solucionar urllib solucionar pushing systematically women smoother plz charles',\n",
       " 'webpage pobre whats workers listeners soviet intentionally passive cuales employ',\n",
       " 'porciones semi zone surely histograms estaban credential seemingly learns unfamiliar',\n",
       " 'parsing brands estimations acquiring summarizing male respected melhor ruined progressively',\n",
       " 'esquemas performing zopa habit eg ayudar constraints war heads distributed',\n",
       " 'tenia solucionar uml parecido entendible invaluable oportunidades tengo clave simplest',\n",
       " 'confusos pet passion ranking featured hot visited capacitaci blue guideline',\n",
       " 'teniendo satisfaction editions cleaner generous determines repetir comandos moves pitched',\n",
       " 'tad functioning apresenta stressed severance entendimento documentations iterations exponentially apresenta',\n",
       " 'maintained excess excessively distracted closer redacci clubbed responsive explicativos debate',\n",
       " 'garbage superfluous correctness transcription narrated refreshment deepen presence troublesome horrendous',\n",
       " 'fica dashboards alternate formation interpreter hong truth offensive nations overcome',\n",
       " 'sejnowski theatre resubmit pathetic stuffs ratios textos cuales organic gb',\n",
       " 'requesting bee stumbling apple constructs lector ignore nao alternate deserved',\n",
       " 'flows generous tackle desarrollar organisation targets formatted horizon opci tackling',\n",
       " 'sports suite aburrido ninguna docs exp auf squares condescending blackboard',\n",
       " 'tad opci crazy intentionally intonation expanded limitation arrays alguna slideshows',\n",
       " 'academically lucky continues produce factors emphasize experimental broaden uniform acces',\n",
       " 'heading abilities clase deploying newly expense behavioral peque advances vedios',\n",
       " 'mathematically affect hey clues marker aprobar lido pratical pleasure plugging',\n",
       " 'cloudera tongue rationale demands shares attacks indicators nociones cats atom',\n",
       " 'ratios intellectually drawbacks remote minutos reas dificiles deploying legit welcome',\n",
       " 'profs birds deals crypto eliminate baked throws roughly quien intervals',\n",
       " 'utterly choppy strengthen entrega ignores bucket labelled gu oil clarifying',\n",
       " 'clouds vedio enlightening peut begginers aussi growth confusos replacement abrupt',\n",
       " 'usan scalable bushee expose lying inform grow docentes contribution fear',\n",
       " 'assingment visited humour contagious academia reduces hunt splitting md ainda',\n",
       " 'stuffs approachable aimed hurdle qa imparte ethereum estos noise btw',\n",
       " 'dreadful compensate algorithmic arent mixing seemingly usable laymen scikit combine',\n",
       " 'waston colocar depended dropping toughest pack internal scheme mid sending',\n",
       " 'factor synthesis strict employed empty entiendo speeds citing atlassian turning',\n",
       " 'interacci justification practising performing connections collect omitted hte reinforcing ignores',\n",
       " 'filtering redesign documented gladly embedding changement martin meta handful estaba',\n",
       " 'arrived encargados cole remote formally tedioso lively affairs curated placed',\n",
       " 'ponen feed dumbed vegan recommender relying designer unhappy counts subjected',\n",
       " 'sampling cryptography reas frames visualisations hammering tweaking importantly tenia consistently',\n",
       " 'puesto simplemente blocked attack qwicklabs yale examining superfluous vous promised',\n",
       " 'extensively acho meal anticipate installations classical japanese dump yale utterly',\n",
       " 'dispon manipulate companion fond py migration continuously polishing teachings hablando',\n",
       " 'schema invested contextual drag reliable conveys estan retroalimentaci tiring describes',\n",
       " 'ratios repetetive transcription verification specialist fleshed fruit greatest pandemic sagemaker',\n",
       " 'margin nutricional pocos registered visto maintain phases empty puesto uml',\n",
       " 'region caveat addressing boot jumbled concentrated removed consult captivating cache',\n",
       " 'artistic admit waston relatora solucionar intonation applets nuanced union complement',\n",
       " 'stopping alternate edition spectroscopy estudiar dairy enabled narrated panorama folders',\n",
       " 'harsh seemingly prevents framing curiosity quien datacamp fond plate jumbled',\n",
       " 'attend scraping uniform initiative segmentation decrease attendees estoy welcomed sensitive',\n",
       " 'quantity predominantly blurred schema newcomer entiendo disagree technicalities commended impractical',\n",
       " 'turns situaciones ppts scrum photographs formed nimo docked qualquer docker',\n",
       " 'grounding nomenclature prototyping earnings decks bonne former lento served inter',\n",
       " 'terwiesch den confirm transmitir variance tenemos decks servers pushing ingredients',\n",
       " 'refers comfort keen oop repitition adopt detailing lightly weights avoiding',\n",
       " 'meanings dependencies huh adopted default confidently filter principals maximize toes',\n",
       " 'equivalent illustrator blue aspecto imagination relaci handbook reminders sleep wrap',\n",
       " 'bite limiting confunde excuse tho brands lectured terwiesch hinted practicas',\n",
       " 'encuentro mastered specificity imc entendible werent packs zip mass modulos',\n",
       " 'scheme centred simplemente tbh norm declared breadth allows deepen coarse',\n",
       " 'esperar aqu validate illustrating supongo performed gis tremendous nonsense center',\n",
       " 'gaining kurs cumple quem workable delay participating sale synthesis trained',\n",
       " 'warning vida firmly awkwardly histograms trained questionnaires dear acquiring allocated',\n",
       " 'tuition decks comply reste materia estas verbatim approfondir grid resolving',\n",
       " 'tem fraudsters misunderstood charting impractical embedding empresas resulta inadequately proving',\n",
       " 'mathematic earth gluten variations methodologies tv electrical max abrupt fraudsters',\n",
       " 'convenient micos empresas speciality bonne determining correctamente wishing md surprising',\n",
       " 'yaakov rios kids cross sugiero departments properties molecular liking intellectually',\n",
       " 'wich melhor hopkins identifying certified sadigov outright practising masters sans',\n",
       " 'adjusted accompanied parecio unexpected auditor budding gripe equity filters closer',\n",
       " 'invested muscle improves leaps estimations estilo tied women databases atom',\n",
       " 'careful pratiques sums opt brought monte toolkit established ciertas entering',\n",
       " 'actively bluej log dificiles imparte gen former profundiza tico prueba',\n",
       " 'figures unaddressed incorporate melhor macros increases unaddressed apes hablar complementario',\n",
       " 'tangible suffice sincerely rote propia financieros preparado hindered fake stops',\n",
       " 'trop unforgiving photos income landscape debiera maintain grid null improvised',\n",
       " 'thousands confuso facial ampliar guest ca forensic picky tl dissapointed',\n",
       " 'hastily ticas penn arroyo latin estimation compilation combinations pulled colocar',\n",
       " 'stopping blackboard viz analyses letters comer determine quietly transcription pmp',\n",
       " 'judging ethereum architectures marker aspecto default iniciarse np neutral reader',\n",
       " 'accents assunto challenged weakness epidemiology shares technicality usable guiding dots',\n",
       " 'quedaron guesswork u passive bag aprobar applicability nuestros sending counts',\n",
       " 'sensitivity imagen victor instrucciones percentage fallacy fixes warm delving record',\n",
       " 'animated picking timing casual squeezed cio patterns remarkable contenu establishing',\n",
       " 'thin observations region calls afternoon locally diving router conducted intersection',\n",
       " 'dollars polishing combine discovered dialogues eyes stress canadian exploratory workings',\n",
       " 'recomendado cambiar narrated movies nuevo analogy addiction meanings deliverables asset',\n",
       " 'scored alguno progressively workbook tom lite passable player applicability overhaul',\n",
       " 'operator marker dissapointed llegar traducci necesidad recognizing exhaustive interpret contradicting',\n",
       " 'maintenance mise pondre send russian healing lightweight noise scholarly conversion',\n",
       " 'inferior asset adquirir adapt react scrape cap invaluable uploading ainda',\n",
       " 'branded timely ich sorting stochastic drawn figures surprised inicio approfondir',\n",
       " 'digging peng necessity employees portal sourced begineers wich actionable qualitative',\n",
       " 'cellphone hecho actionable repeatedly modified graphical espec unprofessional deserves outlined',\n",
       " 'knock estas implications cards intresting ps acting califican wishes inclusive',\n",
       " 'discrete tedioso readable ride efectivo excited lightly enhancing lady vet',\n",
       " 'lesser algumas empresa schema row succinct reflective breve aclarar variance',\n",
       " 'vi evening estilo obliga noticeable smoother drag macros guitar datacamp',\n",
       " 'verify entendimiento modification earth increasing fly surprising compulsory differ device',\n",
       " 'circle outright tweet concerning esa modified technicality solidify entendimento pratique',\n",
       " 'prescriptive trouv precious neo gladly profundiza invaluable fp haphazardly uploaded',\n",
       " 'hacerse workbooks everywhere gut calm disheartening incluso co algumas powershell',\n",
       " 'steven category parents stopping mess smoothly client communities operator viendo',\n",
       " 'readers crazy lstms jokes translations iterations utile asleep stat py',\n",
       " 'turns hectic adjust macroeconomics applicability sas lights codecademy holes rank',\n",
       " 'subscribe complementar trough specializations shorten carlo offense separator beginer monetize',\n",
       " 'presentacion recursos desperately pude enrich complains attacks acho functionalities null',\n",
       " 'root farther dear recovery visualisation suit satisfaction debiera accomplishment syntaxes',\n",
       " 'lograr akin respected bocconi gb comprised nutritional safari air leer',\n",
       " 'countless terwiesch mise selecting entries rminos mic louder startups dsx',\n",
       " 'adecuado speeds synced reaching repaso ish mettre fear fin fly',\n",
       " 'blindly fluent redone qualidade picks torna especial combinations interpretation redone',\n",
       " 'iterations masters tutores estimation internal moderation recession recognize esquemas outright',\n",
       " 'generously convince zoom ipv mixing childish introductive society verified meanings',\n",
       " 'scrum terminal definite quiklab modo plugging scheme codeacademy cheat meaningless',\n",
       " 'functioning comer egger enormous rote quelques doe graduates messed continuation',\n",
       " 'rectify registered existed tour principiante filmed existe escp scoping programacion',\n",
       " 'faint worry owners competent retention wrangling theoretically schema confusas pod',\n",
       " 'llega timeline meals attempted wholly practiced additions africa cuenta tweaking',\n",
       " 'bibliograf troublesome molecular callbacks leen progressing begins mission unit aims',\n",
       " 'coarse observed remotely profesora somos frames pose recruiting michael cash',\n",
       " 'bi toolkit uploaded citations notch tiempos estaba revisited handbook decimal',\n",
       " 'klout additions advises ordinary massive addiction essence effectiveness muddled winded',\n",
       " 'ramp assimilate drastically opaque aspiring laws dia intonation intervenants equal',\n",
       " 'approached principiantes allotted fundamentally approached linkedin ubuntu practiced innovative amplio',\n",
       " 'preparaci curated respeto beliefs jumbled exponer bucket mucha dollar universities',\n",
       " 'confirmation pod contextual med performing concisely operational academia intervenants filmed',\n",
       " 'extended den gaining collaborative previos favorite retail h confortable encompasses',\n",
       " 'frustrations concisely aways disagree uniform dragging dedication legitimate meh hurdle',\n",
       " 'workshops facing bag pretrained failure loaded troublesome bonne harry dislike',\n",
       " 'met sugiero restarted meu achei proceed optimize readers infra evaluar',\n",
       " 'picky loud bibliography noise deliverables foot cheers justified np espera',\n",
       " 'downer generous entrepreneurship assigment shiller recomendar definite sector undergrad hammering',\n",
       " 'responsible status act lockdown surprised capable illustrated relied valid streaming',\n",
       " 'faq russian overhaul trax located snippets legitimate verify calculating unenrolled',\n",
       " 'barbara flash cnicas assign moves conflicting hiring parecido gluten seguridad',\n",
       " 'folks accepted imaging shares executing spectroscopy negatives captivating russian lol',\n",
       " 'blame strictly ruins segmentation crashing sas travel styles truth gr',\n",
       " 'evaluar outlining disabled proving referencing dislike apes practicas practitioner cancel',\n",
       " 'comprender inserted registered obliga resilience tremendous rio duda nobody simulation',\n",
       " 'dev authority ish instantly titles meu owners shorten faulty barbara',\n",
       " 'inability orientado haya acabo evaluated mindset iteration respeto circle especial',\n",
       " 'vie geral nociones century preview geral discrete mechanisms plate entitled',\n",
       " 'indirect robot quando unstructured ich innovative correctamente priority akin monotonic',\n",
       " 'fitness workload spectroscopy owns remotely repository tenga cheers gu spectroscopy',\n",
       " 'replaced cohesive sejnowski empirical estimations pocos uc semaines documentations espero',\n",
       " 'connections seligman fundamentos reas cuestiones dragging ellos translating erd replication',\n",
       " 'storming pulled derivatives asset espero docker invaluable careful attitude entries',\n",
       " 'arrived tune likes thereby wanna nombre flight ficas estilo operational',\n",
       " 'dieron recommender citations auditor comparative outils systematically executing ensuring solvable',\n",
       " 'capacitaci bullet validity imparten dealt mundo vie overhaul indirect suivre',\n",
       " 'walking quedaron differentiation respective brilliantly launched fora beautiful scripted ayudar',\n",
       " 'objetivo attend pv collect bibliograf meals tense infographics virtually ciertos',\n",
       " 'applets assess prescriptive shut enjoying cleaned ethical lesser cour delay',\n",
       " 'tangible dsx confunde optimal alta moderator cuesti turning squeeze ses',\n",
       " 'beliefs senior werent client thumbs manipulating detracted ricos tour architectures',\n",
       " 'limits diploma advances inactive estaba lol inbuilt whatsoever nuevas british',\n",
       " 'hubiese colleague charged pushing barbara monitor intuitively fo instrument pregunta',\n",
       " 'disabled viendo implied arranged deals annoyed visited translated tomar hypothetical',\n",
       " 'streaming ingredients correcci dyslexia puzzle careers aiming fuel synthesis opened',\n",
       " 'involving gratis ensinar differentiate gb explicados careful cultures cient repo',\n",
       " 'pet listas teached returned supposedly determining night redes russian sped',\n",
       " 'agency preferir choses baja respuesta quotes libro sketchy unexperienced imc',\n",
       " 'scraping ordered lesser gu vital nuevamente null primeros garbage lento',\n",
       " 'overlap redesign basa proceeding news picos alta crashing crazy requests',\n",
       " 'ruin max sharp ademas interviewing obesity pharmaceutical empathy humour cualquier',\n",
       " 'plugging examenes ficos safe nas ratios clever totalmente remarks remedial',\n",
       " 'assunto absence sirven modification keeping aulas revisi frustrate chrome fear',\n",
       " 'generation scraping flows performing wolves veo specifics proved assertions informa',\n",
       " 'reflection browsers reset mir u robert pool usages macos greek',\n",
       " 'presenting minus x seconds profundidad stuffs isso leyendo additions cv',\n",
       " 'clv knn herramienta contribution ninguna thousand prototyping counter puzzled send',\n",
       " 'frustrations translator default misunderstanding www pinyin bot sido connections continued',\n",
       " 'geometric mantener toma flash ted hardest iniciarse h medida disparity',\n",
       " 'skimmed toes filtering biz tune funci detailing tv necesidad tip',\n",
       " 'habits achei boy applies consumer virtually macroeconomics linkedin verification discrete',\n",
       " 'acabo faut cv parec practitioner assure participantes bibliography leader molecular',\n",
       " 'rote canadian critique translations device misspellings road decimal refreshed worthless',\n",
       " 'paul chrome dairy processo afternoon translator util engineers inter impactful',\n",
       " 'computations coordination pipelines qualify alguna knowlege tour resultados strictly evident',\n",
       " 'inefficient navigation capable dognition cue handbook tre formulate subtleties flexible',\n",
       " 'justified proceso substantially vegetarian quicklabs organisation inefficient hang drag backend',\n",
       " 'desperately predict depths syntaxes np globally fly considerably repository coaching',\n",
       " 'misunderstandings ratios reflective spectroscopy maximize united explicativos outils oportunidades hacia',\n",
       " 'challenged estaba troubles pursuing www harsh necesario cotejo outlining georgia',\n",
       " 'compensate indices sped overpriced mildly neatly political explican quem helpfull',\n",
       " 'bloom gladly personnel woman alta centered necessity justified contribute grounding',\n",
       " 'ayudan seligman downhill easiest parecio reminder returned clumsy presumably aprobar',\n",
       " 'income legitimate estoy launched clues hurdle convenience trough conclusive war',\n",
       " 'strengths hit confirmation troubleshoot individual servers holds streaming macbook knowledgable',\n",
       " 'estrellas empresas dumbed comma unprepared cheat scrape employed indepth capital',\n",
       " 'pursuing tampoco combinations effects algorithmic inference stupid identical purposes launched',\n",
       " 'touching patients pool cole parecido addiction utilizar dsx translating produce',\n",
       " 'resultado classifier calls acquainted raising intonation framing verification shell ne',\n",
       " 'gb delves practicals diagram evidencia fund mins guy launched presenta',\n",
       " 'screenshots famous meaningless keyword tv transformation navigation intent presentar yale',\n",
       " 'utilizing dives empathy informa certificates explicaban agradecer empresa undergrad hablando',\n",
       " 'doable boy ruim hypothetical exponentially iniciar examining refers ciertas dtu',\n",
       " 'llegar elevado predictions verification virtually outlines family cap adjustments spinning',\n",
       " 'explainations definite salud niveau category bore nuances begging ca losing',\n",
       " 'jam arts providers integral frustrate elevado qwiklab entiendo alta inference',\n",
       " 'troublesome verdad advices peque aller uploaded exploring mcqs shots util',\n",
       " 'curated proofreading fala browsers thereby geometric dumbed validated medida recently',\n",
       " 'vous immense rote assimilate calculated apresenta circle challenged digestible vain',\n",
       " 'profesional sourced accepting netamente docked classifier arts countless quisiera mill',\n",
       " 'donot sides tad especifica outright clv tenido overcome initiating calculated',\n",
       " 'formulation brian toma vital pocos plagiarized pedantic checks verifying forth',\n",
       " 'expectativas boot conventions portfolio interactions actors ipv british conceitos operate',\n",
       " 'sencillos lite incentive teams clicking faut negocio schema hammering pulled',\n",
       " 'pagar educative implications casual allerdings inexperienced monkey sale estoy exhausting',\n",
       " 'academia doy h nuggets computador card maintain inbuilt convince factor',\n",
       " 'curated payment climate semicolons mapreduce foram programacion titulo lesser flows',\n",
       " 'fortunately assist badge vid excess startups edici optimize neutral satisfaction',\n",
       " 'hacerlo segmentation crypto responding cuales ordered qwiklab ltimo tediosos wich',\n",
       " 'misspellings utilities complementarias aclarar casual productivity ii attributes mechanics row',\n",
       " 'plans assingment executing entorno machines meeting renewable quarter assign acredito',\n",
       " 'actors faint canvas translated nutricional filosof stressful keyboard troubles cited',\n",
       " 'llevar funcionamiento visto workings maximum incentive recruiting relaci cr legitimacy',\n",
       " 'util increases verified firefox quest respective cleared matem ceased voz',\n",
       " 'participaci iterations agregar impart sophisticated judge entregar coherence misses convenient',\n",
       " 'recruitment repaso banking curiosity mastering opaque famous situational drugs overlap',\n",
       " 'aprendidos bluej feet exclusively fraction extracted parsing genuine talkative accompanied',\n",
       " 'cap especialmente combine promotional qualify compelling invaluable passion ins educated',\n",
       " 'debate bibliography usable interacci female disease delay misspellings immediate bonne',\n",
       " 'refinement posible careers copious practico nao monitoring realidad practicas subt',\n",
       " 'lite employer limit simulator postings hypothetical consciente optimization ans escucha',\n",
       " 'multivariate tutores continuity crazy pytorch dear varias owns descent accomplished',\n",
       " 'rid sobretodo regressions payoff requested senior puedan passes steven figures',\n",
       " 'differentiation stumbling aparte anatomy veo evidenced panda sincerely quiere bland',\n",
       " 'rationale tiring guessing redundancy hated actively norm usefulness throws sides',\n",
       " 'careful decades scene assesment blind unknown folks docker disorganized callbacks',\n",
       " 'outlook directories theoritical polish dt infra npv transparent iterations pratical',\n",
       " 'surprise loaded router servers desk eval rica estas dreadful trained',\n",
       " 'overviews principio hollywood expectativas modified oportunidades worthless efficiency stopped puesto',\n",
       " 'programmed cards broadly newcomer hint verbiage softwares assuming hassle moma',\n",
       " 'tracking captured occur albeit nutritional optimal troublesome engine unaddressed polishing',\n",
       " 'proud inactive automation juypter expectativas monologue prescriptive pratical possibility gen',\n",
       " 'distance py satisfy folders fader foo collected meanings begining competent',\n",
       " 'loads tongue estilo mouse critic dataprep harry elaborating intent stuffs',\n",
       " 'squeeze messed ratios onwards purely hte assez bash kill promises',\n",
       " 'installed toolkit hectic preferir approved effectiveness doable jokes trend desarrollo',\n",
       " 'locally plugging prescriptive center tuition obliga rminos confidently sujet zanzibar',\n",
       " 'callbacks nutricional sequences owner approfondir def fluid aprend strokes slice',\n",
       " 'controller designing asi translating legit ich postings standalone backing nick',\n",
       " 'card sandbox comandos uva disappeared drag crashing empirical fruit entregar',\n",
       " 'zu greatest formation contextual requisite pursuing existen consistently vezes player',\n",
       " 'comienzo strictly practicality melhorar declared fragmented upcoming electrical michel convnets',\n",
       " 'tenia ech avanzado photographers interactions controversial iniciar inicial collection pertaining',\n",
       " 'choppy quarter teorico strangely chat continuous er def affordable knife',\n",
       " 'legitimate tutoriales upgraded cybersecurity hot choses authority uc respectful deo',\n",
       " 'kill orientation outputs estos anaconda voire lite solvable quantity conditional',\n",
       " 'evaluado abandoned executives hyperparameter handful urge pull gram instantly nuggets',\n",
       " 'dollar handling consume ambiguas backend investigaci implied reader kimberley contagious',\n",
       " 'profundiza demuestra losing exceptionally descent seguido internalizing asap algebraic worksheets',\n",
       " 'clues volver visually misses gen imposible tab sums personality privacy',\n",
       " 'paragraph opposite established scant elas construct tag phase iniciar climate',\n",
       " 'journal characteristics gente horrendous practicar knn charting dedication knn bi',\n",
       " 'pedantic factual penn analyst success explican approved diapositivas qualified appris',\n",
       " 'spoiler unos scarce instrument interfaces aprobar muestra espero safari congratulations',\n",
       " 'moma anecdotes imagen frauds dsx pausing ne quotes switching responding',\n",
       " 'systematically impart exchange electrical quem disorganized dumbed attend presumably pedag',\n",
       " 'regressions ipv responsible selected md academics panda theoretically necesita estaba',\n",
       " 'inefficient unavailable utilities reliance embeddings grinding mechanically manipulating nuggets couse',\n",
       " 'manufacturing pouco careful poorer famous clients maximum advantages vegan intervals',\n",
       " 'pudieran consistency analyst partly penn performing ipv studied umich dump',\n",
       " 'x delved llega textual refreshed conduite differ menus hugely investors',\n",
       " 'attending fits bonne pues loaded aquellos yellow boy stops mental',\n",
       " 'co coping incluir fruitful requested significa wich praise mismatches hacerlo',\n",
       " 'construct educated spoiler stands antes retake translations empieza wich muita',\n",
       " 'investigar labeled robot unsure desafiante sus warrant melhor emphasizing begginer',\n",
       " 'vastly statistic packaged packaged senti partial potencial streaming exponentially pc',\n",
       " 'cohort climate sonia quien emerging divided ipad vet workflow struck',\n",
       " 'max quem tomar members slice examen abandoned rl inappropriate gentle',\n",
       " 'anticipate minded throw offensive aparte mes valuations outright diseases intentionally',\n",
       " 'goood indentation reporting cohort automation triplet loses customers hamper assunto',\n",
       " 'embora suspect delete puzzled neatly synthesis vedio hot tbh molecular',\n",
       " 'throws sized mnist prevents collaborate hypothetical outlined permiten employees holds',\n",
       " 'relying ayudo shell wild notion mejora attitude bu bag horrendous',\n",
       " 'card disabled japanese propia lifestyle climate brian responsible privilege rambling',\n",
       " 'remotely analyst hyperparameters upgrading rehearse consistently parents philosophers validated juypter',\n",
       " 'profs creator legitimate behaviour hac chain fundamentally informal presentados entirety',\n",
       " 'operational negocios crystal ipad slicing budding resubmit sweet svm aburrido',\n",
       " 'consistently specialty siguientes syntactical meta ejercicio cotejo iterative mining modulos',\n",
       " 'specifications fragmented fragmented entirety sensitive visto scant confirm countless gina',\n",
       " 'alimentaci senti resultado architectures cert sums reciting gram dropping especifica',\n",
       " 'dollar lugar assignements vectorisation graduates sens dissapointed soporte allocated saludo',\n",
       " 'repeatedly operational coach extension helpless summarized assistants chris cert restart',\n",
       " 'purely mindset hub centered contenu weeds broadly gladly expense replication',\n",
       " 'recourse pense mixture prueba scalable improves proofread renewable remembering cyber',\n",
       " 'lives beliefs silly explores variance fuentes pasado stream dato disheartening',\n",
       " 'tait uniformity deserved prewritten cross launched depending formulation rater render',\n",
       " 'bag aprobar rewarding propose framed tas dear array stimulating blog',\n",
       " 'disheartening raj entendimento exploring conduite comparisons comprehensible browsers unforgiving likes',\n",
       " 'optimum affects legitimate nns nations pandemic employees reduced recommened harsh',\n",
       " 'pruebas datos extenso educators consist programme brands enrol mining acquire',\n",
       " 'throws guesswork restrictions elderly cognition enriching equity soviet polished lightly',\n",
       " 'masters indirectly fear microsoft worried debate limits provider keeping bloom',\n",
       " 'continued scene introdu sped mapreduce autism newcomers privilege letting adam',\n",
       " 'ball specialized contextual avoiding courser combination standpoint fund explaination quantity',\n",
       " 'vain goood arent sicas dado cramped ensuring utilizar toma recomendar',\n",
       " 'ignoring amateur golang commented decisiones transcription pretrained tidbits dispon names',\n",
       " 'qgis revisar monitoring paul counts innovative misconceptions antes plagiarized dealt',\n",
       " 'sooooo entregar expense deliverable bluej proofreading matrices represented abordar redesign',\n",
       " 'tenemos opposite visto bluej practitioner cairo animation diapositivas inactive card',\n",
       " 'stock round ipv engineers torna tenemos canvas financials accepting morning',\n",
       " 'congratulations victims mouse acknowledge syntaxes toma notch visualisation cotejo compulsory',\n",
       " 'verbiage ainda muita restriction fora sequences phases berger misunderstood welcome',\n",
       " 'healthier earned principio unenrolled overlap adjust united generate estaban aspiring',\n",
       " 'exclusively fancy implications fragments acerca nutella anchor nick certified practitioner',\n",
       " 'acredito encouraged recommendable objects trough questionnaires rectify classifier dito citations',\n",
       " 'tico blue tout visited np heads locally evaluated navigation species',\n",
       " 'keyboard boost summarize educated relatora carry device cleaner sirven replace',\n",
       " 'increases institute keyword measure timelines helper soviet critique ello earned',\n",
       " 'brush merits sean hablar respectful candidates overpriced investors dialogue secuencia',\n",
       " 'baja energetic assimilate recomendado senti courseera sean pobre suficiente remotely',\n",
       " 'thousands precisely reputable enormous informati shares referente fundamentally basa formally',\n",
       " 'l mixing proceeds behaviors fundamentos record desperately profundiza inform confirmation',\n",
       " 'ball exhaustive pronunciations complement shell united chat gmail asi frustrante',\n",
       " 'bite pretrained empirical fixing quien previo nociones practicos ar wrangling',\n",
       " 'inferential ricas notable facilidad graduates stimulate disappeared increases sucks interactivo',\n",
       " 'pollan physical cpu mastering discouraged male customers strictly promised modo',\n",
       " 'verdad reminders quiklab minus experimental operating upto balancing mismos complejos',\n",
       " 'callbacks blah quelques asd sparked learns communities recognized ricas passively',\n",
       " 'ramps retail structuring photographers kill clever translated introductive increases store',\n",
       " 'choses igual fresh spaced continuous mastering skeptical subjective inputs inferior',\n",
       " 'biggest snippets filters recommender visited produce presumably glossary attack lively',\n",
       " 'carrying informacion semicolons debate tag misspellings lake interacci tom workings',\n",
       " 'exigentes estos undergrad ac obsolete tempo keyboard bonne mismatch continuity',\n",
       " 'utile slowed assertions joining carelessly ne undergrad lightweight demuestra improvised',\n",
       " 'describe canadian hopeful executives rater voices computations quem tackle hare',\n",
       " 'arguably documentations lesser apt pandemic doc bolts stands oportunidades bubble',\n",
       " 'nowadays responding keyword ruin filter involves ish knowledgable forces misled',\n",
       " 'beliefs ma behavioural extension meanwhile spirit experi remaining describes heck',\n",
       " 'gradually unhelpful candidate limitation fragmented generales converted segunda crafted werent',\n",
       " 'cio reminder troublesome easiest ruins ciertas inspirational moderate tied beaucoup',\n",
       " 'convenience circle employed presentar male emergency pf worksheets responsible cultural',\n",
       " 'reproduce voz legal neatly streaming nobody mindshift agrado night macos',\n",
       " 'aids cuesti repetitivo printing ritmo theoritical africa agency blow emphasizing',\n",
       " 'exemplos adapted execute meanwhile py breakdown combinations proud ignores logically',\n",
       " 'assure film interacci extract dashboards locally arbitration deploying assunto overviews',\n",
       " 'generate pursuing modelos extensions sciences garbage forgotten weaknesses lee passive',\n",
       " 'africa trough hammered revert sumar explicado virtually impractical excersices arrived',\n",
       " 'entendiendo arrived enhancing permet filter indices comer analyses comprehensible qa',\n",
       " 'cache customized posibilidad troublesome prescriptive rehearse existen pobre comprehended wtp',\n",
       " 'subpar toma unhelpful brochure analogy entrega practiced strategic empresas favor',\n",
       " 'entrega overpriced veterinarian tl distance feeding toughest conducive revisited express',\n",
       " 'dito variation suffering century angry trend charged imc cc visualizing',\n",
       " 'crystal isso confundir differentiation actualizar british adopted dirigido cloudera explicaron',\n",
       " 'rushing shape predict quem relationships flows hecho delved ongoing dtu',\n",
       " 'contemporary dreadful ellas nonsense opener partly propia esfuerzo gulf verbiage',\n",
       " 'compulsory simulator haya trabajar distraction practiced forming puzzle society junior',\n",
       " 'sequential european contradictory grey utilidad bushee beautifulsoup realidad hung orientado',\n",
       " 'filtering rubrics heck workaround deserved doy primera skims dificuldades comprehensible',\n",
       " 'occasion claras everywhere callback alguno situational roll outlines waited feed',\n",
       " 'negatives timed accompany mcq equity excessive profiles processo labelled terwiesch',\n",
       " 'carries maintenance managing searches unique throwing misspellings emotional ds impacted',\n",
       " 'poorer males automation verbatim construct muitas hub kimberley crashed empresas',\n",
       " 'measure actualizado vedios anatomy tenia calculated adicionales sencillos movies brushing',\n",
       " 'apt excessively edx everywhere dispon mildly beliefs sampling scene citing',\n",
       " 'mastering researched builds evolves credentials responded excercise patients client ish',\n",
       " 'carrying demotivating ratos comma amateur ruined applicability startups unfocused immense',\n",
       " 'authority iceberg beautifulsoup confortable obtener cairo negocio filling participate suffering',\n",
       " 'tomar inappropriate letting disparity kudos deserved remote possibilities structural arts',\n",
       " 'font reworked chose spyder u comentarios lucid claramente reducing tactics',\n",
       " 'participaci puntos precisely algumas nimo gas toes store nations flag',\n",
       " 'macbook facial illustrating commonly dissapointed expansions critic pull synthesis italian',\n",
       " 'buttons essence translate film lengua salt interviewed tenemos ruined puzzled',\n",
       " 'carrying gulf erd expires uploading keeping canada satisfaction ont vectorized',\n",
       " 'geometric equipped critics puedan india gaining comer standpoint graduation rooted',\n",
       " 'pointing importantes diapositivas administrators attended vectorized cv faith combine principals',\n",
       " 'nowadays reporting passable puzzles llamar stopping frankly virtually blocked microphone',\n",
       " 'medio channel explicativos heading specializations melhor vie commitment postings abordar',\n",
       " 'keys aspecto rio counted keyboard attitude investors cotejo registered string',\n",
       " 'menus simplest journalism employer berger robot pasado rehash certificates imagination',\n",
       " 'plagiarized unidades entend conversational approfondir akin nscht profundo inferior communities',\n",
       " 'specificity compile dilemma adds peace identifying pivot intellectual appropriately cash',\n",
       " 'axis intuitively executives tweet remote vectorized apt differentiate generalized learns',\n",
       " 'mapreduce peoples doctor dumbed arts medida definiciones opened protocols developments',\n",
       " 'teorico executives styles canvas normally collect llega imparten workaround sustainability',\n",
       " 'jenkins accompanied educative citations minutos rambling society assesment eclipse delays',\n",
       " 'decimal california esquemas citing correlations tuviera assingment beginers hypothetical courseera',\n",
       " 'calculated hopkins sensitivity expose purchased nail credentials exception rigour complementarias',\n",
       " 'nonsense jokes organisation gce zanzibar boot act textos unintuitive coder',\n",
       " 'bummer falto achei addiction meditation bluej justification claims facial repaso',\n",
       " 'throws pedantic cultures propaganda questionnaires costo zip raw convention curse',\n",
       " 'accents variations zip senti practitioner netamente diseases apes zopa bitcoin',\n",
       " 'priority cheaper attempted organisation docker nitty atom qwiklab alumno escrita',\n",
       " 'oil saving solidify predictions wolves overcome label percentage docs ac',\n",
       " 'nuanced perspectiva unengaging tuples reforzar hablando equity adjust landscape sabe',\n",
       " 'ma semantics guiding spirit crafted recall quedaron desperately reflecting permite',\n",
       " 'mismos sas onwards horse identical fell summarizing factor humble rios',\n",
       " 'lovely hindered understands flowing downloads designs disagree tablet hunt surprised',\n",
       " 'satisfy pausing pedagogy conversational realidad logging solidify voire normally evolves',\n",
       " 'vectorization sumar western proving boost anza convinced arrived hugely sans',\n",
       " 'longest forming shortcomings attended conventions chrome hyperparameter inter personality pfa',\n",
       " 'relying supplemented acces architect young impractical molecular understands intervenants bore',\n",
       " 'pobres paso experimentation factual musical hugely crashes sudden puedan bridge',\n",
       " 'imagery reliance intermediates clubbed crafted pedagogy interfaces backing hare node',\n",
       " 'certaines totalmente def procedure employed conveys glimpse adquirir sequences enviar',\n",
       " 'tenga cash arroyo collaborate genius ridiculously smoother mining prototyping ordinary',\n",
       " 'offensive aims k excelentes qualified faint arent conversational documentations consists',\n",
       " 'utilizar scarce road nations evidenced expanding superior empirical invaluable squeezed',\n",
       " 'cramped essence eclipse k former util multiplication tense totalmente tbh',\n",
       " 'layman researchers guitar embarrassing werent debes contextual seg forecasting dirigido',\n",
       " 'ceased victor filler instrument performing familiarize began captured hrs ds',\n",
       " 'jeff permet confidently complained flexible optimize callback chris suite occurs',\n",
       " 'profundo unos perspectiva accompany monotonic artistic touching passion comply fuentes',\n",
       " 'backwards exercies metric ups shall pocos edx brings theoretic courseware',\n",
       " 'tab empresas operational importantes peoples zoom expectativa gulf illustrator adjust',\n",
       " 'evaluado agradezco puesto behaviors advances pm roi lengua progression remedial',\n",
       " 'incluso worthy resolving absoluto plugging rigorousness computation plagiarized fuentes ppts',\n",
       " 'synopsis practicar preferir naturally edx laboratorios verification boot talkative negocio',\n",
       " 'planes responded habit vastly blue contribution minus deals ein formulation',\n",
       " 'unstructured hardest headaches regulations optimize significance aprendidos motivate cats superior',\n",
       " 'fica occasion leyendo shell claras assignements encargados menes assigned employees',\n",
       " 'externa partly hacker ofrece junior jupyterlab conventions canada postings guiding',\n",
       " 'solidify estrategias mechanical documentary intersection micro cheers aprendidos blocked apt',\n",
       " 'weights anatomy freely abrupt replace recommending index macos certificaci simultaneously',\n",
       " 'restriction outstanding namely flaw handful specialist firmly recommending colors alive',\n",
       " 'imc advantages frames eliminate documented negocio flying comer alternate agrees',\n",
       " 'luego instruments retained optimize ma unforgiving hall executives elegir peoples',\n",
       " 'letting shortcuts centricity begining crappy lightweight lives preprocessing criticisms dataframes',\n",
       " 'respective pulled udio bi desarrollar guiding saving trend recommened meandering',\n",
       " 'sas drag hacerlo sas puesto consumer documentary softwares employ night',\n",
       " 'understands sparse estado unfocused cc painfully essence sees architect suggesting',\n",
       " 'zeppelin merged co inferior repo utilities grid comma fond robert',\n",
       " 'faculties restriction bravo existed pedagogy empirical worthless principiantes propose convenience',\n",
       " 'capacitaci hacerse forecasting solamente procedure interacting blame obliga register lista',\n",
       " 'u lawyer substantial electrical channels registered silly conduite cards typically',\n",
       " 'intervenants cpu scale encima authority regurgitation fewer qwicklabs grande aprendizado',\n",
       " 'yoga apples dissapointing captions excessive convert compile squeeze hacia serait',\n",
       " 'descent elegir necesita modifying pearson graduates cheat demonstrates carrying rank',\n",
       " 'shiller realidad passive superb responsible bees provoking formulate eclipse technicalities',\n",
       " 'conte supongo potencial idioma loving architect graphical inclusion ciertas bulleted',\n",
       " 'drastically vital legitimate stretch divide occur conocer infra advantage retake',\n",
       " 'campo importing buttons meal panda classmate travel individually reminded sencillos',\n",
       " 'aims subscribe datalab dashboards calificaciones estamos sensitive newcomer pitched existent',\n",
       " 'superfluous movies naturally overpriced squeeze certificaci academical unpolished iniciarse carrying',\n",
       " 'anyhow robot durante ponen float criticisms abbreviated virtually frames margin',\n",
       " 'notations backprop fluffy oil uk k stopping occurs zone polish',\n",
       " 'rooms foo classifier inaccurate requested captivating variability spectroscopy renamed nobody',\n",
       " 'fora storytelling deviate harry billed foot align western peculiar scroll',\n",
       " 'www contextual profs definite accompanied respectful landscape max instrucciones analogy',\n",
       " 'ranking bird mnist repetitivo rutina authority opener dicen edx compulsory',\n",
       " 'cultures diets navigation apartado virtually gce ne counter intriguing rios',\n",
       " 'usuarios relied soporte enlightening maneras bright conhecimento lets formulate stackflow',\n",
       " 'objetivo lee implied eliminate responding efectivo utilizar alguna resubmit initiating',\n",
       " 'struggles org routing needlessly modify vital differentiate builds pena oil',\n",
       " 'differs microsoft dataframes clients backpropagation exercised comer brings ein demotivating',\n",
       " 'sourced graphical fluffy recognize invested didactic namely tour tv supply',\n",
       " 'docentes interspersed dots recursos veterinarian keys eeuu yellow commonsense typed',\n",
       " 'discrete periods mindset healing acquainted novices neo sorts enrich consumer',\n",
       " 'linking sale alexander produce wet estrategias lengthen ins ignoring spare',\n",
       " 'privacy penalized quarter status devote semi asap brings existed occasion',\n",
       " 'alimentos delays private doctor apesar anaconda alter anza matem academics',\n",
       " 'entendible practicals estava sum hablar sean blocks demonstrating institutions puntos',\n",
       " 'recruiting fuel cheating justification titulo outlook hastily boost begining atom',\n",
       " 'actualizaci courseware filters spelled cuestiones leader billed responsive xml cancel',\n",
       " 'manipulate item torna gradual scan figured crystal loud faut conflicting',\n",
       " 'tracks tutoriales cnicas entregar separator bite thick chat male indices',\n",
       " 'apologies hub nail locations intros supplemented geral ficou portfolio u',\n",
       " 'equipped loads hung earth patients fortunately broaden torna beating determines',\n",
       " 'fraction loads drawing concisely viewed house universal misspellings mba utilizar',\n",
       " 'occurred transition drink hac illustrator estos pocos verification medicine claims',\n",
       " 'bot sigue cnicas profissionais tiempos ethereum tab confidently wanna abandoned',\n",
       " 'bloom microphone earnings practiced highlights wow prep young stepik akin',\n",
       " 'l forming correctness tenga toes typographic classifier pocos delving decades',\n",
       " 'grab substantial measured confusas portal hicieron estrategias allotted feet hub',\n",
       " 'georgia meanings claim ran escrito performing recommending pollan brian administrators',\n",
       " 'compensate muscle doctor compulsory cc certified estava factored ish unsupervised',\n",
       " 'encargados unexperienced fake tuition insert ejercicio foo variation applets neglected',\n",
       " 'arte utilities newcomers ricos notable stressful maximize drawing picking dataframes',\n",
       " 'practica returns nuestros sciences farther presumably doable persona incorporate additions',\n",
       " 'crashes flash immense posed esfuerzo lograr resilience audiences verbiage fico',\n",
       " 'decisiones dp chemicals resubmit established filter van official strangely combinations',\n",
       " 'uploading encouraging strict stops iceberg necesarios artistic convnet annoyed filosof',\n",
       " 'ti distraction logically quien confortable coder desk encryption doblaje token',\n",
       " 'society implies thumbs fighting verified summarised captured downhill opini unengaging',\n",
       " 'akin tengo participant cairo analogy notion tought contrary assign firstly',\n",
       " 'advances strings dataprep accomplishment solver cairo affected grip seguridad divide',\n",
       " 'descargar contribute recommendable populations ceis dictionaries empresa lockdown player fico',\n",
       " 'opener desirable incomprehensible semicolons screens eyes redacci precisa depended brands',\n",
       " 'anchor servers strangely territory customers demanded kurs stopping mixture british',\n",
       " 'soviet reflecting pulled variance akin construct determines exponentially pubsub qualitative',\n",
       " 'minded assure amounts quoted incompetent easiest photographs observation tas llega',\n",
       " 'ir cited habit bridge lying ordinary heads classified influence unengaging',\n",
       " 'supposedly jupiter narration qwicklabs knowledges wall credibility interspersed acronym ans',\n",
       " 'apple causing unpolished struggles calls grow underwhelming iniciar indirectly invaluable',\n",
       " 'concisely sklearn aclarar guesswork commended categories proofread capture matched freely',\n",
       " 'begineer exigentes transmitir dulos collab lento tenses entering audible educators',\n",
       " 'mixture convince macos qualquer divide pa sided accompany hablando department',\n",
       " 'resulta index solvable translations amplio ben responsible tying throw hashtag',\n",
       " 'pessoas applets sincerely arena primeros opt complementarias mala represented puzzled',\n",
       " 'preguntan younger labelled matem servers bucket congratulations walkthroughs disabled somethings',\n",
       " 'viewed removed resort pharmaceutical behavioural addresses layman captivating py latin',\n",
       " 'crafted digos remain complementarias explainations stands terwiesch glossary proposed helpfull',\n",
       " 'easiest explicaron scale supplies estad crazy checks workers beautifully license',\n",
       " 'assunto explainations separator logging addiction locally venture narrated apes convince',\n",
       " 'classic act pushing loads importing educators alter rows pobre assunto',\n",
       " 'dialogue variations beliefs throws convention relationships desktop compelling guideline describes',\n",
       " 'remarks income h induction rationale tarefas dataprep illustrate fueran letters',\n",
       " 'situational couse urban scroll intern square reliance utilities launch descargar',\n",
       " 'bore recession suit bar restrictions occurs fro quelques sensaci menus',\n",
       " 'ubuntu calm translator primera snippets monologue sirvi sobretodo honor nns',\n",
       " 'smoother allotted requested recipe folder documentary virtually potentially learns readers',\n",
       " 'palabras unfortunate sustaining praised bi preference unlock extensively speeds das',\n",
       " 'fee russian respective trees succinct fueran mapreduce diagram personality ipad',\n",
       " 'competition vastly hubiese deliberately pretrained esa vividly standpoint actors tackle',\n",
       " 'unforgiving contributions stellar banking candidates stock shots loads increases comparisons',\n",
       " 'analyses remembered enrich malo isso simplicity foram equity variation reminder',\n",
       " 'technicalities tenemos translations computador agradezco altered encouragement trop googled mit',\n",
       " 'clubbed sorted stopping acknowledge interacting interacci sketchy empathy yeah wordy',\n",
       " 'empresas align repositories generalized appreciation panda morning qued learns decks',\n",
       " 'considerations sido lowest convolution acho habla importing performed courseera rio',\n",
       " 'dollar cats cnicas driving installations vaguely reallly begginer aquellos downsides',\n",
       " 'hung autism sus artistic units evolution bird harsh indices curiosity',\n",
       " 'proved unbearable solver familiarizado variance investors lightweight evaluado dato hyperparameter',\n",
       " 'cependant electric justification parecido continuous provoking prospective detalles repetir startups',\n",
       " 'interacci mildly complejos hacerse strengthen md documentations varied yo institutions',\n",
       " 'seventh suspect africa baja esos monetize train stochastic dialogue tl',\n",
       " 'permiten frequentist comprender male participantes reflecting dnn leaps solidify arts',\n",
       " 'intervenants figured md picks british assure lexical relatable loving fala',\n",
       " 'narrated normally displeased documentations downloads educative unbearable herself webcasts puedes',\n",
       " 'hyperparameter kill exp manually usuarios helper aims lengthen frames ont',\n",
       " 'primera refinement sehr polish physics font countless definite government udemy',\n",
       " 'dollars rank hood verification sampling toronto ses preview strings btw',\n",
       " 'incomprehensible pool candidate guideline oportunidades responsible gods resulta purchase transcriptions',\n",
       " 'fin rl charge visualisations fica unavailable ben lite structuring negocio',\n",
       " 'reported flaw wordy pytorch repo foro unengaging judge nowadays courseera',\n",
       " 'managing detracted scoring convolution jam correcci learnings hacerlo ses leadership',\n",
       " 'qualidade embedding misled frustrante crypto shiller pushed artistic discovered ups',\n",
       " 'kill funky tour sports nuevas versed hunt educative werent font',\n",
       " 'backprop conhecimento arent splitting mentioning ltimo naive jam confunde relaxed',\n",
       " 'amplio outlook smoother faint np bag thru mine incluir ses',\n",
       " 'aulas achei harm assist documented graphical grid repaso yoga preparado',\n",
       " 'folders ma resolving overcome ich necessity tought ourse gripping facial',\n",
       " 'imparten fitting verdad nuggets unos apresenta phrases ambiguas multiplication card',\n",
       " 'letting vegan wrap operator expose leyendo laundry dutch profissionais structural',\n",
       " 'recommending channels recovery shall scored entregar polished loosely explicados travel',\n",
       " 'tackling imo eval cache fluid keyword concisely entendiendo letter count',\n",
       " 'reflective blue poorer complementario na mother attack inspirational senior advantages',\n",
       " 'eat tout resourses scripting wholly mba upgrading conceptually repetitivo crafted',\n",
       " 'freely powerpoints matters ca spaced datacamp davis documented kubernetes fundamentally',\n",
       " 'spot compatible debiera separator credibility contemporary aulas forecasting rubrics utile',\n",
       " 'fuzzy repeatedly entered earth ethical japanese esos diets designing gladly',\n",
       " 'ricas judge send presentados formas recap reciting bibliography assistants watered',\n",
       " 'disease presumably tight assistants government dichos wrangling confusos trained recently',\n",
       " 'famous weeds pratiques soft ricos walked continuous delving category occurred',\n",
       " 'exhaustive eval earth servers wound thinks validation scikit playground remedial',\n",
       " 'applicability thousands fewer slice unos row whomever utilizing cet practica',\n",
       " 'overloaded century catalog favorite allotted awarded claras film uc cairo',\n",
       " 'breeze modifying speeds square quem gradually carries positives investigaci llevan',\n",
       " 'abarca index hamper locations losing preference detracted voz easiest confuse',\n",
       " 'correcting represented informal overhaul rational breakdown mining letras baked solidify',\n",
       " 'reforzar scour embarrassing accomplished dashboards stale scattered overhaul choses superb',\n",
       " 'correlate chris actors leans thick bu conversational nutricional increasing googled',\n",
       " 'excessively skilled verified worthless sacred dtu addresses dire bonne roll',\n",
       " 'lights binomial consistently acknowledge unhelpful exp toughest voiceover minutos cuesti',\n",
       " 'pide principalmente pun mba resultados tense compute stellar needing pytorch',\n",
       " 'er obtuse squeeze gente procedures qualidade screenshot suit sima misled',\n",
       " 'unintentionally unforgiving imparte pienso token sent droit firstly nutricional cat',\n",
       " 'revisions pool l auditor suspect investors artwork tbh private act',\n",
       " 'ciertas constructs loaded storage gratis faltaron promotional pido young wishes',\n",
       " 'greek uninteresting lag nuanced pod callbacks qualities struggles reallly monotonic',\n",
       " 'estava sheer ninguna trend retail beating encouragement vain inter pertaining',\n",
       " 'resoluci courseware textbooks usual encounter thumb carlo frustrations drawbacks pronunciations',\n",
       " 'tiempos satisfaction distraction attempted varying arrays variations egger jeff scandinavia',\n",
       " 'minimally caveat forming artistic hte young insert sampling upgraded mismos',\n",
       " 'investigar preguntan exponer cop everytime toes conducted sem variation imparted',\n",
       " 'educator poster arranged leaps meanings bibliography kimberley knn urllib dialogues',\n",
       " 'imc u afternoon continuation mainstream toughest overpriced keyword slips opened',\n",
       " 'inexperienced male obesidad nimo mix diseases formulation stiff queda erd',\n",
       " 'qued preseed benefitted childish hated conducive stale clustering begining dedication',\n",
       " 'infographics diverse creatively tout semantics justification sehr simplify fuerza ont',\n",
       " 'emphasizing dyslexia md contribution slideshows pseudocode sugiero totalmente academia supports',\n",
       " 'conflicting journalism merge totalmente competent assesments proficiency tait quit unos',\n",
       " 'preguntan badge redes peoples satisfy moderation werent cancel produce intellectual',\n",
       " 'reallly encontrar stone shape longest minutos implies totalmente attacks agregar',\n",
       " 'seg dise habit standpoint simultaneously fuentes cambiar actualizar mastered interactivo',\n",
       " 'observation digested robots refining regressions iniciarse purchasing trop llevar acquire',\n",
       " 'blue companion skin meh refreshment scarce teams existe plagiarized claim',\n",
       " 'kahn painfully modulo coherent contextual dire brought melhor thumbs owns',\n",
       " 'actively processo choses location identical yeah inter crashing raised oversight',\n",
       " 'private deliverable bite executing charting necesarios appearing backprop necesarios roll',\n",
       " 'margin dyslexia imparten anaconda describes friends reporting commentary lively validity',\n",
       " 'fight empresarial strengths didactics wordy generales laboratory detrimental hashtag nombre',\n",
       " 'greatest argument cole authority diapositivas authority joined ensuring scant compelling',\n",
       " 'regulations restriction counter unavailable dramatically finanzas delves healing ses categorized',\n",
       " 'dots ramps thumbs water ans seeker prevalent highest healing bummer',\n",
       " 'senior mix estava estos backing tip faulty march labelled exemplos',\n",
       " 'impacts pipelines monetize env geral separator neo conclusions quicklabs appearing',\n",
       " 'establish reallly zopa overload scoring teorico validation fight tremendous somos',\n",
       " 'western disadvantage graduated investors monkey foros dificil published progressively roi',\n",
       " 'scenes partial caliber carrying baked wrangling iniciar optimize relaci squeezed',\n",
       " 'opci ciertas menes desear absence estudiante suffering quisiera docked demotivating',\n",
       " 'ip availability senti constructs amplio visualisation lowest wrangling aussi batna',\n",
       " 'mejora operate desperately necessity factors travel profs pack candidate nuanced',\n",
       " 'predict workarounds michel opaque send ruined requests abarca bu beware',\n",
       " 'metodologias stochastic lento incorporate perception player exp semicolons owing california',\n",
       " 'gluten fraudsters certains framing dealt arrived beginers graduation preview sobretodo',\n",
       " 'assure stone tuples adicionales approved bash indepth programacion desperately convenience',\n",
       " 'acquiring iceberg delhi egypt llega mejora basa senti rework entiende',\n",
       " 'safe intervenants nociones narrator expectativas peque pleased repetitivo empathy continuously',\n",
       " 'misspellings gru framed hashtag preprocessing customers vital hospital facial stiff',\n",
       " 'faltaron convnets classified surprisingly misconceptions hecho grinding repository inexperienced entenderlo',\n",
       " 'primeros practiced quiere cloudera dirigido adam vegan parecido evidenced responded',\n",
       " 'folders potentially installed inicial horrendous orique cap supports npv evaluado',\n",
       " 'paso practicals ignoring linkedin entiendo depths stimulate requested aspiring explicaron',\n",
       " 'shiller crashed accomplishment impart fits bocconi postings causing fifth night',\n",
       " 'hablando optimal anterior obliga sirve bloom yea policy originated tive',\n",
       " 'relationships libro tout wholly replacing hall solidify bot resulta tampoco',\n",
       " 'buying cramped exhausting yellow kids venture minus gender center rushes',\n",
       " 'finicky redacci container situational arrived comprender gonna essentials british clumsy',\n",
       " 'pocos aburrido menes serait replacing npv esquemas coordinated nomenclature exceeded',\n",
       " 'hot alternatives encryption entregar affordable systematically uninspired fancy remedial horse',\n",
       " 'float specialist subpar consistently confidently gew bonus porciones flying metodolog',\n",
       " 'preferably thinks reminder fruitful preguntan abandoned losing entrepreneurship incluso suficiente',\n",
       " 'valuation deadline organisation pursuing authority scheme instagram handwritten represented tematica',\n",
       " 'promotional younger educator relaxed paso sehr pairs applies golang departments',\n",
       " 'consistently channel uploading preparado obstante prioritization sistema ed beware transition',\n",
       " 'calculated entenderlo nao evident thousands checks figures paul misplaced properties',\n",
       " 'programme interpreter frustrations bag monotonic buzzing unfortunate powershell unprofessional relations',\n",
       " 'payed acquire designer extenso totalmente practicals apresenta universities teams estilo',\n",
       " 'seulement mettre manejo newly altogether remedial haphazardly brinda docente diseases',\n",
       " 'cela foo lograr sensaci stuffs overload affordable entirety minded evidente',\n",
       " 'hastily convnn dangerous panel peoples bridge dumbed x joined agradecer',\n",
       " 'africa embedding pseudocode differentiate validate autism promised dictionaries counts vedio',\n",
       " 'spreadsheet ruim anterior fund investors sums recordings alguna breakdown regularization',\n",
       " 'juypter graduates pense store launching exception arte totalmente journal cotejo',\n",
       " 'powerpoint letters recordings rework philosophers entendible birds skimming implies specificity',\n",
       " 'prep cope reputation remedial existed kick cert impractical humour cc',\n",
       " 'convenient esperar privacy proposed conversational gentle timers stock toughest arent',\n",
       " 'dognition mill alguna struck beginers allot specialist oriques requested terwiesch',\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5211ea92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81970</th>\n",
       "      <td>Probably the best financial market course on c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81971</th>\n",
       "      <td>From a knowledge perspective, I have learned a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81972</th>\n",
       "      <td>Some parts of the course were informative.  So...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81973</th>\n",
       "      <td>The course is not that challenging, it could b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81974</th>\n",
       "      <td>It's more of a course for the people who have ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews  sentiment\n",
       "81970  Probably the best financial market course on c...          0\n",
       "81971  From a knowledge perspective, I have learned a...          0\n",
       "81972  Some parts of the course were informative.  So...          0\n",
       "81973  The course is not that challenging, it could b...          0\n",
       "81974  It's more of a course for the people who have ...          0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c45b4024",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = pd.DataFrame(data_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93f7d3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tossed resultados routing downhill axis narrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>region apt crashes flight transmitir leadershi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>specifics redacci aburrido rooms desarrollar c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weaknesses dissapointed ca didactics marker vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yaakov qualidade fader asi conocer figures art...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  tossed resultados routing downhill axis narrat...\n",
       "1  region apt crashes flight transmitir leadershi...\n",
       "2  specifics redacci aburrido rooms desarrollar c...\n",
       "3  weaknesses dissapointed ca didactics marker vi...\n",
       "4  yaakov qualidade fader asi conocer figures art..."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ec7d1fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-84-98a1ab77489f>:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  total_data = data.append(aug_data)\n"
     ]
    }
   ],
   "source": [
    "total_data = data.append(aug_data)\n",
    "total_data = total_data[[\"reviews\", \"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35156bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81970</th>\n",
       "      <td>Probably the best financial market course on c...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81971</th>\n",
       "      <td>From a knowledge perspective, I have learned a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81972</th>\n",
       "      <td>Some parts of the course were informative.  So...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81973</th>\n",
       "      <td>The course is not that challenging, it could b...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81974</th>\n",
       "      <td>It's more of a course for the people who have ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews  sentiment\n",
       "81970  Probably the best financial market course on c...        0.0\n",
       "81971  From a knowledge perspective, I have learned a...        0.0\n",
       "81972  Some parts of the course were informative.  So...        0.0\n",
       "81973  The course is not that challenging, it could b...        0.0\n",
       "81974  It's more of a course for the people who have ...        0.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "be485b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58292, 2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8b209e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data = pd.read_csv(\"./og_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "91e382b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map stars to sentiment\n",
    "def map_sentiment(stars_received):\n",
    "    if stars_received <= 2:\n",
    "        return -1\n",
    "    elif stars_received == 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "# Mapping stars to sentiment into three categories\n",
    "og_data['sentiment'] = [map_sentiment(x) for x in og_data['rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e5cbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = og_data[og_data.sentiment==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ca1eecc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(989056, 3)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b88cfe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data = og_data[og_data.sentiment==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "da560807",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_neg_data = pos_data.shape[0] - neg_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "87e99d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "965415"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "66e49c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for i in range(req_neg_data):\n",
    "    z = tf.random.normal(shape=(1,10), mean=5000, stddev=2500)\n",
    "    z = np.array(z).astype(int)\n",
    "    z = np.reshape(z, 10)\n",
    "    new_data.append(tokenizer.sequences_to_texts([z]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d66a9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_append = []\n",
    "for i in new_data:\n",
    "    data_to_append.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "09572ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = pd.DataFrame(data_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b3adcb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-116-f91ede5a605d>:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  total_data = og_data.append(aug_data)\n"
     ]
    }
   ],
   "source": [
    "total_data = og_data.append(aug_data)\n",
    "total_data = total_data[[\"reviews\", \"sentiment\"]]\n",
    "total_data = total_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fbc73021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(total_data['reviews'], total_data['sentiment'], test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fa90e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern= r'\\b\\w+\\b')\n",
    "train_matrix = vectorizer.fit_transform(x_train)\n",
    "test_matrix = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "be7b2bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "af21abbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(train_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8deaaa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf.predict(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "59353eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.85      0.99      0.91      6109\n",
      "         0.0       0.83      0.98      0.90      9134\n",
      "         1.0       1.00      0.99      1.00    299298\n",
      "\n",
      "    accuracy                           0.99    314541\n",
      "   macro avg       0.89      0.99      0.94    314541\n",
      "weighted avg       0.99      0.99      0.99    314541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(preds,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "84eb4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c0050c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_gan.pkl\", \"wb\") as file:\n",
    "    pkl.dump(rf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7f32a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vectorizer_gan.pkl\", \"wb\") as file:\n",
    "    pkl.dump(vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5509f4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_15 (Sequential)  (None, 1000)              1540584   \n",
      "                                                                 \n",
      " sequential_16 (Sequential)  (None, 1)                 259713    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,800,297\n",
      "Trainable params: 1,800,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "390de2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, None, 128)         1280000   \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1000)              129000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,540,584\n",
      "Trainable params: 1,540,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "69fc0221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, None, 128)         128000    \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 259,713\n",
      "Trainable params: 259,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de3c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
